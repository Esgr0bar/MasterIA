{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AI-Based Audio Mixing and Mastering","text":""},{"location":"#overview","title":"Overview","text":"<p>This project aims to develop an AI-powered tool for automated audio mixing and mastering. The tool leverages machine learning algorithms to analyze and replicate the effects applied to reference tracks, enabling rapid processing of audio files.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Automated Mixing and Mastering: AI-driven analysis and application of audio effects.</li> <li>Flexible Adjustment: Users can choose from different effect intensities and manually tweak the results.</li> <li>Support for Various Genres: The AI can be calibrated to handle different music genres, starting with rap.</li> </ul>"},{"location":"#project-structure","title":"Project Structure","text":"<ul> <li>Data Processing: Functions for loading and preparing audio data.</li> <li>Feature Extraction: Methods to extract useful features like MFCCs and spectrograms.</li> <li>Model Training: Scripts to build, train, and evaluate machine learning models.</li> <li>User Interface: A simple application interface (to be developed).</li> </ul> <p>Explore the documentation to learn more about how to use the tool and contribute to the project.</p>"},{"location":"api_reference/","title":"API Reference","text":"<p>This section contains the detailed API documentation for the project's Python modules.</p>"},{"location":"api_reference/#data-processing","title":"Data Processing","text":""},{"location":"api_reference/#data_processing.load_audio_files","title":"<code>load_audio_files(directory)</code>","text":"<p>Loads multiple audio files from a directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Path to the directory containing audio files.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary where keys are filenames and values are numpy arrays of audio data.</p>"},{"location":"api_reference/#data_processing.load_audio_files_with_metadata","title":"<code>load_audio_files_with_metadata(directory)</code>","text":"<p>Loads multiple audio files along with their metadata.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Path to the directory containing audio files and metadata.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary where keys are filenames and values are tuples (audio data, metadata).</p>"},{"location":"api_reference/#data_processing.split_tracks","title":"<code>split_tracks(audio_data, segment_length=5)</code>","text":"<p>Splits multiple audio tracks into segments.</p> <p>Parameters:</p> Name Type Description Default <code>audio_data</code> <code>dict</code> <p>Dictionary of audio data where keys are filenames.</p> required <code>segment_length</code> <code>int</code> <p>Length of each segment in seconds.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary with segmented audio data.</p>"},{"location":"api_reference/#feature-extraction","title":"Feature Extraction","text":""},{"location":"api_reference/#feature_extraction.extract_basic_features","title":"<code>extract_basic_features(audio_data)</code>","text":"<p>Extracts basic audio features (e.g., spectral, loudness) from the audio tracks.</p> <p>Parameters:</p> Name Type Description Default <code>audio_data</code> <code>dict</code> <p>Dictionary of audio data.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Dictionary of extracted features.</p>"},{"location":"api_reference/#feature_extraction.extract_mfcc","title":"<code>extract_mfcc(audio_data, n_mfcc=13)</code>","text":"<p>Extract MFCC features from multiple audio tracks.</p> <p>Parameters:</p> Name Type Description Default <code>audio_data</code> <code>dict</code> <p>Dictionary of audio data where keys are filenames.</p> required <code>n_mfcc</code> <code>int</code> <p>Number of MFCCs to return.</p> <code>13</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary with MFCC features.</p>"},{"location":"api_reference/#feature_extraction.extract_spectrogram","title":"<code>extract_spectrogram(audio_data)</code>","text":"<p>Extract spectrogram features from multiple audio tracks.</p> <p>Parameters:</p> Name Type Description Default <code>audio_data</code> <code>dict</code> <p>Dictionary of audio data where keys are filenames.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary with spectrogram features.</p>"},{"location":"api_reference/#model-training","title":"Model Training","text":""},{"location":"api_reference/#model_training.prepare_data_for_training","title":"<code>prepare_data_for_training(features, audio_data)</code>","text":"<p>Prepares data for training by associating features with metadata labels.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>dict</code> <p>Extracted features for each file.</p> required <code>audio_data</code> <code>dict</code> <p>Audio data with corresponding metadata.</p> required <p>Returns:</p> Type Description <p>X, y: Features matrix and label array for training.</p>"},{"location":"api_reference/#model_training.train_action_prediction_model","title":"<code>train_action_prediction_model(X, y)</code>","text":"<p>Trains a model to predict actions based on features.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>Feature matrix.</p> required <code>y</code> <code>array</code> <p>Label array.</p> required <p>Returns:</p> Name Type Description <code>model</code> <p>Trained classifier model.</p>"},{"location":"api_reference/#model_training.train_model","title":"<code>train_model(feature_data, labels)</code>","text":"<p>Trains an ensemble model on multiple audio tracks with CNN and voting classifier.</p> <p>Parameters:</p> Name Type Description Default <code>feature_data</code> <code>dict</code> <p>Dictionary of features where keys are filenames.</p> required <code>labels</code> <code>dict</code> <p>Dictionary of labels corresponding to the features.</p> required <p>Returns:</p> Name Type Description <code>model</code> <p>Trained ensemble model.</p>"},{"location":"api_reference/#inference_1","title":"Inference","text":""},{"location":"api_reference/#inference.load_model","title":"<code>load_model(model_path)</code>","text":"<p>Load the pre-trained machine learning model.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the pre-trained model file.</p> required <p>Returns:</p> Name Type Description <code>model</code> <p>The loaded model object.</p>"},{"location":"api_reference/#inference.predict_actions","title":"<code>predict_actions(model, audio_data)</code>","text":"<p>Predicts the actions and cuts for the given audio data.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The pre-trained machine learning model.</p> required <code>audio_data</code> <code>dict</code> <p>Dictionary where keys are filenames and values are audio data.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Suggested actions and cuts for each audio file.</p>"},{"location":"api_reference/#inference.run_inference","title":"<code>run_inference(model_path, audio_data)</code>","text":"<p>Run the inference process on the provided audio data.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the pre-trained model file.</p> required <code>audio_data</code> <code>dict</code> <p>Dictionary where keys are filenames and values are audio data.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Suggested actions and cuts for each audio file.</p>"},{"location":"api_reference/#action-suggestion","title":"Action Suggestion","text":""},{"location":"api_reference/#action_suggestion.print_suggested_actions","title":"<code>print_suggested_actions(actions)</code>","text":"<p>Prints out the suggested actions for each track.</p> <p>Parameters:</p> Name Type Description Default <code>actions</code> <code>dict</code> <p>Suggested actions dictionary.</p> required"},{"location":"api_reference/#action_suggestion.suggest_actions","title":"<code>suggest_actions(model, features)</code>","text":"<p>Suggests actions based on model predictions.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>Trained model.</p> required <code>features</code> <p>Extracted features from new audio.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Suggested actions for each track.</p>"},{"location":"api_reference/#action_suggestion.suggest_cuts","title":"<code>suggest_cuts(model, features)</code>","text":"<p>Suggests creative cuts or glitches in the beats.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>Trained machine learning model.</p> required <code>features</code> <code>dict</code> <p>Extracted features from new audio tracks.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Suggested cuts or modifications.</p>"},{"location":"api_reference/#feedback_1","title":"Feedback","text":""},{"location":"api_reference/#feedback.collect_user_feedback","title":"<code>collect_user_feedback(suggested_actions, suggested_cuts)</code>","text":"<p>Collect user feedback on the suggested actions and cuts.</p> <p>Parameters:</p> Name Type Description Default <code>suggested_actions</code> <code>dict</code> <p>The actions suggested by the AI.</p> required <code>suggested_cuts</code> <code>dict</code> <p>The cuts suggested by the AI.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>A list of feedback entries, where each entry is a dictionary.</p>"},{"location":"api_reference/#feedback.incorporate_feedback_into_training","title":"<code>incorporate_feedback_into_training(feature_data, labels, feedback_file='feedback.json')</code>","text":"<p>Incorporates user feedback into the training process to refine the model.</p> <p>Parameters:</p> Name Type Description Default <code>feature_data</code> <code>dict</code> <p>Dictionary of features where keys are filenames.</p> required <code>labels</code> <code>dict</code> <p>Dictionary of labels corresponding to the features.</p> required <code>feedback_file</code> <code>str</code> <p>Path to the file containing user feedback.</p> <code>'feedback.json'</code> <p>Returns:</p> Name Type Description <code>model</code> <p>Retrained machine learning model.</p>"},{"location":"api_reference/#feedback.save_feedback","title":"<code>save_feedback(feedback, filename='feedback.json')</code>","text":"<p>Saves user feedback to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>feedback</code> <code>list</code> <p>List of feedback entries.</p> required <code>filename</code> <code>str</code> <p>The name of the file where feedback will be saved.</p> <code>'feedback.json'</code>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p>"},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Initial project structure with <code>data_processing.py</code>, <code>feature_extraction.py</code>, and <code>model_training.py</code>.</li> <li>Basic unit tests for all modules.</li> <li>Jupyter notebooks for EDA and model training.</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>N/A</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>N/A</li> </ul>"},{"location":"changelog/#010-2024-08-09","title":"[0.1.0] - 2024-08-09","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Initial setup of <code>mkdocs</code> for project documentation.</li> </ul>"},{"location":"ci_cd/","title":"Continuous Integration and Deployment","text":""},{"location":"ci_cd/#overview","title":"Overview","text":"<p>We use continuous integration (CI) to automatically test and deploy our project. This ensures that new changes do not break existing functionality and that our documentation is always up-to-date.</p>"},{"location":"ci_cd/#github-actions","title":"GitHub Actions","text":"<p>Our project is set up with GitHub Actions for CI/CD. The following workflows are defined:</p> <ul> <li>Test Workflow: Automatically runs the unit tests defined in the <code>tests/</code> directory.</li> <li>Documentation Deployment: Deploys the <code>mkdocs</code> documentation to GitHub Pages whenever changes are pushed to the <code>main</code> branch.</li> </ul>"},{"location":"ci_cd/#setting-up-cicd","title":"Setting Up CI/CD","text":"<ol> <li>Test Workflow:</li> <li>The test workflow runs <code>pytest</code> to execute all unit tests.</li> <li> <p>Ensure all tests pass before merging to <code>main</code>.</p> </li> <li> <p>Documentation Deployment:</p> </li> <li><code>mkdocs gh-deploy</code> is used to deploy the latest documentation to GitHub Pages.</li> </ol>"},{"location":"ci_cd/#workflow-files","title":"Workflow Files","text":"<p>You can find the CI workflow files in the <code>.github/workflows/</code> directory.</p> <p>For more details on setting up and customizing CI/CD workflows, refer to the GitHub Actions Documentation.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>We welcome contributions to improve this project. Whether you want to report a bug, suggest an enhancement, or submit code, here\u2019s how you can contribute:</p>"},{"location":"contributing/#how-to-contribute","title":"How to Contribute","text":"<ol> <li>Fork the Repository:</li> <li> <p>Fork the project on GitHub to your account.</p> </li> <li> <p>Clone the Repository:</p> </li> <li> <p>Clone your fork locally with:      <code>bash      git clone https://github.com/your-username/your-repo-name.git</code></p> </li> <li> <p>Create a New Branch:</p> </li> <li> <p>Create a branch for your changes:      <code>bash      git checkout -b feature/my-feature</code></p> </li> <li> <p>Make Changes:</p> </li> <li>Implement your changes or additions.</li> <li> <p>Add tests if applicable.</p> </li> <li> <p>Commit and Push:</p> </li> <li> <p>Commit your changes and push them to your fork:      <code>bash      git commit -m \"Description of changes\"      git push origin feature/my-feature</code></p> </li> <li> <p>Create a Pull Request:</p> </li> <li>Submit a pull request to the main repository.</li> <li>Ensure your pull request description clearly explains the changes and references any relevant issues.</li> </ol> <p>Thank you for your contributions!</p>"},{"location":"data/","title":"Data Handling","text":""},{"location":"data/#data-structure","title":"Data Structure","text":"<p>The project works with audio data stored in a specific directory structure:</p> <ul> <li>Raw Data: Unprocessed audio files, typically in <code>.wav</code> format.</li> <li><code>data/raw/tracks/</code>: Contains folders for each track (e.g., <code>song1/</code>, <code>song2/</code>).</li> <li> <p>Example: <code>data/raw/tracks/song1/vocals.wav</code></p> </li> <li> <p>Processed Data: Feature files and preprocessed data ready for model input.</p> </li> <li><code>data/processed/features/</code>: Contains extracted features like MFCCs and spectrograms.</li> <li>Example: <code>data/processed/features/song1_vocals_mfcc.npy</code></li> </ul>"},{"location":"data/#data-collection","title":"Data Collection","text":"<ul> <li>Source Audio: Collect raw audio files for different tracks (vocals, instruments, etc.).</li> <li>Reference Tracks: Include mixed and mastered tracks for the AI to learn from.</li> </ul>"},{"location":"data/#data-processing-workflow","title":"Data Processing Workflow","text":"<ol> <li>Loading Audio:</li> <li> <p>Use the <code>load_audio()</code> function from <code>src/data_processing.py</code> to load audio files into numpy arrays.</p> </li> <li> <p>Splitting Tracks:</p> </li> <li> <p>Use the <code>split_tracks()</code> function to divide longer audio files into manageable segments.</p> </li> <li> <p>Feature Extraction:</p> </li> <li> <p>Use <code>extract_mfcc()</code> and <code>extract_spectrogram()</code> from <code>src/feature_extraction.py</code> to extract features for model training.</p> </li> <li> <p>Saving Processed Data:</p> </li> <li>Save extracted features as <code>.npy</code> files in the <code>data/processed/features/</code> directory.</li> </ol> <p>Ensure your data is properly organized before starting model training.</p>"},{"location":"inference/","title":"Inference Script Documentation","text":""},{"location":"inference/#overview","title":"Overview","text":"<p>The <code>inference.py</code> script is designed to perform inference on new audio data using a pre-trained machine learning model. It suggests actions and creative cuts to be applied to the audio tracks.</p>"},{"location":"inference/#functions","title":"Functions","text":""},{"location":"inference/#load_modelmodel_path","title":"<code>load_model(model_path)</code>","text":"<ul> <li>Description: Loads the pre-trained machine learning model from the specified path.</li> <li>Arguments:</li> <li><code>model_path</code> (str): Path to the model file.</li> <li>Returns: The loaded model.</li> </ul>"},{"location":"inference/#predict_actionsmodel-audio_data","title":"<code>predict_actions(model, audio_data)</code>","text":"<ul> <li>Description: Predicts the actions and cuts for the given audio data using the pre-trained model.</li> <li>Arguments:</li> <li><code>model</code>: The pre-trained machine learning model.</li> <li><code>audio_data</code> (dict): Dictionary where keys are filenames and values are audio data.</li> <li>Returns: A dictionary with suggested actions and cuts for each audio file.</li> </ul>"},{"location":"inference/#run_inferencemodel_path-audio_data","title":"<code>run_inference(model_path, audio_data)</code>","text":"<ul> <li>Description: The main function that runs the inference process. It loads the model, predicts the actions, and returns the results.</li> <li>Arguments:</li> <li><code>model_path</code> (str): Path to the pre-trained model file.</li> <li><code>audio_data</code> (dict): Dictionary where keys are filenames and values are audio data.</li> <li>Returns: A dictionary with suggested actions and cuts for each audio file.</li> </ul>"},{"location":"inference/#usage","title":"Usage","text":"<p>```bash python inference.py --model_path \"path/to/saved/model.pkl\" --audio_data \"path/to/audio/files\"</p>"},{"location":"notebooks/","title":"Jupyter Notebooks","text":""},{"location":"notebooks/#overview","title":"Overview","text":"<p>The Jupyter notebooks included in this project are designed for data exploration and model training. They provide an interactive environment where you can test different approaches and visualize the results.</p>"},{"location":"notebooks/#available-notebooks","title":"Available Notebooks","text":"<ul> <li>EDA.ipynb: Explore the raw audio data, visualize waveforms, spectrograms, and MFCCs.</li> <li>Model_Training.ipynb: Build and train machine learning models for audio processing.</li> </ul>"},{"location":"notebooks/#how-to-use","title":"How to Use","text":"<ol> <li>Set Up Your Environment:</li> <li>Ensure that you have all dependencies installed by running <code>pip install -r requirements.txt</code>.</li> <li> <p>Start Jupyter with the command <code>jupyter notebook</code>.</p> </li> <li> <p>Open a Notebook:</p> </li> <li>Navigate to the <code>notebooks/</code> directory.</li> <li> <p>Open <code>EDA.ipynb</code> to start exploring the data or <code>Model_Training.ipynb</code> to begin training models.</p> </li> <li> <p>Run Cells:</p> </li> <li>Execute code cells by selecting them and pressing <code>Shift + Enter</code>.</li> <li> <p>Modify parameters as needed to experiment with different settings.</p> </li> <li> <p>Save Your Work:</p> </li> <li>Don\u2019t forget to save your notebook regularly. You can also export it as HTML or PDF for sharing.</li> </ol> <p>Explore the notebooks to get hands-on experience with the data and the models!</p>"},{"location":"usage/","title":"Usage Guide","text":""},{"location":"usage/#getting-started","title":"Getting Started","text":"<p>This guide will help you use the AI-Based Audio Mixing and Mastering tool effectively.</p>"},{"location":"usage/#installation","title":"Installation","text":"<ol> <li> <p>Clone the repository:    <code>bash    git clone https://github.com/Esgr0bar/MasterIA.git    cd MasterIA</code></p> </li> <li> <p>Install dependencies:    <code>bash    pip install -r requirements.txt</code></p> </li> </ol>"},{"location":"usage/#basic-usage","title":"Basic Usage","text":"<p>The main entry point is the <code>main.py</code> script:</p> <pre><code>python main.py\n</code></pre>"},{"location":"usage/#processing-audio-files","title":"Processing Audio Files","text":"<ol> <li>Prepare your audio data:</li> <li>Place raw audio files in the <code>data/raw/tracks/</code> directory</li> <li> <p>Each track should be in its own subdirectory</p> </li> <li> <p>Extract features:</p> </li> <li>Use the feature extraction module to process your audio files</li> <li> <p>Features will be saved in <code>data/processed/features/</code></p> </li> <li> <p>Train models:</p> </li> <li>Use the model training module to create your AI models</li> <li> <p>Models will be saved in <code>data/processed/models/</code></p> </li> <li> <p>Run inference:</p> </li> <li>Use the inference module to apply effects to new audio files</li> </ol>"},{"location":"usage/#advanced-usage","title":"Advanced Usage","text":"<p>For more advanced usage, refer to the API Reference section and the Jupyter notebooks in the <code>notebooks/</code> directory.</p>"},{"location":"reference/data_processing/","title":"Data Processing","text":""},{"location":"reference/data_processing/#data_processing.load_audio_files","title":"<code>load_audio_files(directory)</code>","text":"<p>Loads multiple audio files from a directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Path to the directory containing audio files.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary where keys are filenames and values are numpy arrays of audio data.</p> Source code in <code>src/data_processing.py</code> <pre><code>def load_audio_files(directory):\n    \"\"\"Loads multiple audio files from a directory.\n\n    Args:\n        directory (str): Path to the directory containing audio files.\n\n    Returns:\n        dict: A dictionary where keys are filenames and values are numpy arrays of audio data.\n    \"\"\"\n    audio_data = {}\n    for filename in os.listdir(directory):\n        if filename.endswith('.wav'):\n            file_path = os.path.join(directory, filename)\n            audio, sr = librosa.load(file_path, sr=None)\n            audio_data[filename] = audio\n    return audio_data\n</code></pre>"},{"location":"reference/data_processing/#data_processing.load_audio_files_with_metadata","title":"<code>load_audio_files_with_metadata(directory)</code>","text":"<p>Loads multiple audio files along with their metadata.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Path to the directory containing audio files and metadata.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary where keys are filenames and values are tuples (audio data, metadata).</p> Source code in <code>src/data_processing.py</code> <pre><code>def load_audio_files_with_metadata(directory):\n    \"\"\"Loads multiple audio files along with their metadata.\n\n    Args:\n        directory (str): Path to the directory containing audio files and metadata.\n\n    Returns:\n        dict: A dictionary where keys are filenames and values are tuples (audio data, metadata).\n    \"\"\"\n    audio_data = {}\n    for filename in os.listdir(directory):\n        if filename.endswith('.wav'):\n            file_path = os.path.join(directory, filename)\n            audio, sr = librosa.load(file_path, sr=None)\n            metadata_file = filename.replace('.wav', '.json')\n            metadata_path = os.path.join(directory, metadata_file)\n            if os.path.exists(metadata_path):\n                with open(metadata_path, 'r') as f:\n                    metadata = json.load(f)\n                audio_data[filename] = (audio, metadata)\n    return audio_data\n</code></pre>"},{"location":"reference/data_processing/#data_processing.split_tracks","title":"<code>split_tracks(audio_data, segment_length=5)</code>","text":"<p>Splits multiple audio tracks into segments.</p> <p>Parameters:</p> Name Type Description Default <code>audio_data</code> <code>dict</code> <p>Dictionary of audio data where keys are filenames.</p> required <code>segment_length</code> <code>int</code> <p>Length of each segment in seconds.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary with segmented audio data.</p> Source code in <code>src/data_processing.py</code> <pre><code>def split_tracks(audio_data, segment_length=5):\n    \"\"\"Splits multiple audio tracks into segments.\n\n    Args:\n        audio_data (dict): Dictionary of audio data where keys are filenames.\n        segment_length (int): Length of each segment in seconds.\n\n    Returns:\n        dict: A dictionary with segmented audio data.\n    \"\"\"\n    segmented_data = {}\n    for filename, audio in audio_data.items():\n        segments = []\n        sr = librosa.get_samplerate(filename)\n        num_samples = sr * segment_length\n        for start in range(0, len(audio), num_samples):\n            segment = audio[start:start + num_samples]\n            segments.append(segment)\n        segmented_data[filename] = segments\n    return segmented_data\n</code></pre>"},{"location":"reference/feature_extraction/","title":"Feature Extraction","text":""},{"location":"reference/feature_extraction/#feature_extraction.extract_basic_features","title":"<code>extract_basic_features(audio_data)</code>","text":"<p>Extracts basic audio features (e.g., spectral, loudness) from the audio tracks.</p> <p>Parameters:</p> Name Type Description Default <code>audio_data</code> <code>dict</code> <p>Dictionary of audio data.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Dictionary of extracted features.</p> Source code in <code>src/feature_extraction.py</code> <pre><code>def extract_basic_features(audio_data):\n    \"\"\"Extracts basic audio features (e.g., spectral, loudness) from the audio tracks.\n\n    Args:\n        audio_data (dict): Dictionary of audio data.\n\n    Returns:\n        dict: Dictionary of extracted features.\n    \"\"\"\n    features = {}\n    for filename, (audio, _) in audio_data.items():\n        sr = librosa.get_samplerate(filename)\n        spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sr)\n        rmse = librosa.feature.rms(y=audio)\n        loudness = np.mean(librosa.feature.spectral_bandwidth(y=audio, sr=sr))\n        features[filename] = {\n            \"spectral_centroid\": np.mean(spectral_centroid),\n            \"rmse\": np.mean(rmse),\n            \"loudness\": loudness\n        }\n    return features\n</code></pre>"},{"location":"reference/feature_extraction/#feature_extraction.extract_mfcc","title":"<code>extract_mfcc(audio_data, n_mfcc=13)</code>","text":"<p>Extract MFCC features from multiple audio tracks.</p> <p>Parameters:</p> Name Type Description Default <code>audio_data</code> <code>dict</code> <p>Dictionary of audio data where keys are filenames.</p> required <code>n_mfcc</code> <code>int</code> <p>Number of MFCCs to return.</p> <code>13</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary with MFCC features.</p> Source code in <code>src/feature_extraction.py</code> <pre><code>def extract_mfcc(audio_data, n_mfcc=13):\n    \"\"\"Extract MFCC features from multiple audio tracks.\n\n    Args:\n        audio_data (dict): Dictionary of audio data where keys are filenames.\n        n_mfcc (int): Number of MFCCs to return.\n\n    Returns:\n        dict: A dictionary with MFCC features.\n    \"\"\"\n    mfcc_features = {}\n    for filename, audio in audio_data.items():\n        mfccs = librosa.feature.mfcc(y=audio, sr=librosa.get_samplerate(filename), n_mfcc=n_mfcc)\n        mfcc_features[filename] = mfccs\n    return mfcc_features\n</code></pre>"},{"location":"reference/feature_extraction/#feature_extraction.extract_spectrogram","title":"<code>extract_spectrogram(audio_data)</code>","text":"<p>Extract spectrogram features from multiple audio tracks.</p> <p>Parameters:</p> Name Type Description Default <code>audio_data</code> <code>dict</code> <p>Dictionary of audio data where keys are filenames.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary with spectrogram features.</p> Source code in <code>src/feature_extraction.py</code> <pre><code>def extract_spectrogram(audio_data):\n    \"\"\"Extract spectrogram features from multiple audio tracks.\n\n    Args:\n        audio_data (dict): Dictionary of audio data where keys are filenames.\n\n    Returns:\n        dict: A dictionary with spectrogram features.\n    \"\"\"\n    spectrograms = {}\n    for filename, audio in audio_data.items():\n        D = np.abs(librosa.stft(audio))**2\n        S = librosa.feature.melspectrogram(S=D, sr=librosa.get_samplerate(filename))\n        spectrograms[filename] = S\n    return spectrograms\n</code></pre>"},{"location":"reference/inference/","title":"Inference","text":""},{"location":"reference/inference/#inference.load_model","title":"<code>load_model(model_path)</code>","text":"<p>Load the pre-trained machine learning model.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the pre-trained model file.</p> required <p>Returns:</p> Name Type Description <code>model</code> <p>The loaded model object.</p> Source code in <code>src/inference.py</code> <pre><code>def load_model(model_path):\n    \"\"\"Load the pre-trained machine learning model.\n\n    Args:\n        model_path (str): Path to the pre-trained model file.\n\n    Returns:\n        model: The loaded model object.\n    \"\"\"\n    return joblib.load(model_path)\n</code></pre>"},{"location":"reference/inference/#inference.predict_actions","title":"<code>predict_actions(model, audio_data)</code>","text":"<p>Predicts the actions and cuts for the given audio data.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The pre-trained machine learning model.</p> required <code>audio_data</code> <code>dict</code> <p>Dictionary where keys are filenames and values are audio data.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Suggested actions and cuts for each audio file.</p> Source code in <code>src/inference.py</code> <pre><code>def predict_actions(model, audio_data):\n    \"\"\"Predicts the actions and cuts for the given audio data.\n\n    Args:\n        model: The pre-trained machine learning model.\n        audio_data (dict): Dictionary where keys are filenames and values are audio data.\n\n    Returns:\n        dict: Suggested actions and cuts for each audio file.\n    \"\"\"\n    features = extract_basic_features(audio_data)\n    suggested_actions = suggest_actions(model, features)\n    suggested_cuts = suggest_cuts(model, features)\n\n    return suggested_actions, suggested_cuts\n</code></pre>"},{"location":"reference/inference/#inference.run_inference","title":"<code>run_inference(model_path, audio_data)</code>","text":"<p>Run the inference process on the provided audio data.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the pre-trained model file.</p> required <code>audio_data</code> <code>dict</code> <p>Dictionary where keys are filenames and values are audio data.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Suggested actions and cuts for each audio file.</p> Source code in <code>src/inference.py</code> <pre><code>def run_inference(model_path, audio_data):\n    \"\"\"Run the inference process on the provided audio data.\n\n    Args:\n        model_path (str): Path to the pre-trained model file.\n        audio_data (dict): Dictionary where keys are filenames and values are audio data.\n\n    Returns:\n        dict: Suggested actions and cuts for each audio file.\n    \"\"\"\n    model = load_model(model_path)\n    return predict_actions(model, audio_data)\n</code></pre>"},{"location":"reference/model_training/","title":"Model Training","text":""},{"location":"reference/model_training/#model_training.prepare_data_for_training","title":"<code>prepare_data_for_training(features, audio_data)</code>","text":"<p>Prepares data for training by associating features with metadata labels.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>dict</code> <p>Extracted features for each file.</p> required <code>audio_data</code> <code>dict</code> <p>Audio data with corresponding metadata.</p> required <p>Returns:</p> Type Description <p>X, y: Features matrix and label array for training.</p> Source code in <code>src/model_training.py</code> <pre><code>def prepare_data_for_training(features, audio_data):\n    \"\"\"Prepares data for training by associating features with metadata labels.\n\n    Args:\n        features (dict): Extracted features for each file.\n        audio_data (dict): Audio data with corresponding metadata.\n\n    Returns:\n        X, y: Features matrix and label array for training.\n    \"\"\"\n    X = []\n    y = []\n    for filename in features.keys():\n        X.append(list(features[filename].values()))\n        y.append(audio_data[filename][1][\"effects\"])  # Assume effects metadata is a list of actions\n    return np.array(X), y\n</code></pre>"},{"location":"reference/model_training/#model_training.train_action_prediction_model","title":"<code>train_action_prediction_model(X, y)</code>","text":"<p>Trains a model to predict actions based on features.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>Feature matrix.</p> required <code>y</code> <code>array</code> <p>Label array.</p> required <p>Returns:</p> Name Type Description <code>model</code> <p>Trained classifier model.</p> Source code in <code>src/model_training.py</code> <pre><code>def train_action_prediction_model(X, y):\n    \"\"\"Trains a model to predict actions based on features.\n\n    Args:\n        X (array): Feature matrix.\n        y (array): Label array.\n\n    Returns:\n        model: Trained classifier model.\n    \"\"\"\n    model = RandomForestClassifier(n_estimators=100)\n    model.fit(X, y)\n    return model\n</code></pre>"},{"location":"reference/model_training/#model_training.train_model","title":"<code>train_model(feature_data, labels)</code>","text":"<p>Trains an ensemble model on multiple audio tracks with CNN and voting classifier.</p> <p>Parameters:</p> Name Type Description Default <code>feature_data</code> <code>dict</code> <p>Dictionary of features where keys are filenames.</p> required <code>labels</code> <code>dict</code> <p>Dictionary of labels corresponding to the features.</p> required <p>Returns:</p> Name Type Description <code>model</code> <p>Trained ensemble model.</p> Source code in <code>src/model_training.py</code> <pre><code>def train_model(feature_data, labels):\n    \"\"\"Trains an ensemble model on multiple audio tracks with CNN and voting classifier.\n\n    Args:\n        feature_data (dict): Dictionary of features where keys are filenames.\n        labels (dict): Dictionary of labels corresponding to the features.\n\n    Returns:\n        model: Trained ensemble model.\n    \"\"\"\n    all_features = []\n    all_labels = []\n\n    for filename in feature_data.keys():\n        features = feature_data[filename]\n        all_features.extend(features.T)  # Transpose to get feature vectors\n        all_labels.extend([labels[filename]] * features.shape[1])\n\n    X_train, X_test, y_train, y_test = train_test_split(np.array(all_features), np.array(all_labels), test_size=0.2)\n\n    # Reshape the data for CNN input (if using 1D audio data, not spectrograms)\n    X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n    X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n\n    # Create the CNN model\n    cnn_model = KerasClassifier(build_fn=create_cnn, input_shape=(X_train_cnn.shape[1], 1), epochs=10, batch_size=32, verbose=0)\n\n    # Create other models\n    rf_model = RandomForestClassifier(n_estimators=100)\n    svm_model = SVC(probability=True)\n\n    # Combine models in a voting classifier\n    ensemble_model = VotingClassifier(estimators=[\n        ('cnn', cnn_model),\n        ('rf', rf_model),\n        ('svm', svm_model)\n    ], voting='soft')  # 'soft' voting uses predicted probabilities\n\n    # Hyperparameter tuning using GridSearchCV\n    param_grid = {\n        'rf__n_estimators': [100, 200],\n        'svm__C': [0.1, 1, 10],\n    }\n\n    grid = GridSearchCV(ensemble_model, param_grid, cv=3, n_jobs=-1)\n    grid.fit(X_train_cnn, y_train)\n\n    best_model = grid.best_estimator_\n\n    # Evaluate the best model on the test set\n    y_pred = best_model.predict(X_test_cnn)\n    print(classification_report(y_test, y_pred))\n\n    return best_model\n</code></pre>"}]}