{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AI-Based Audio Mixing and Mastering","text":""},{"location":"#overview","title":"Overview","text":"<p>This project aims to develop an AI-powered tool for automated audio mixing and mastering. The tool leverages machine learning algorithms to analyze and replicate the effects applied to reference tracks, enabling rapid processing of audio files.</p>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code># Clone the repository\ngit clone https://github.com/Esgr0bar/MasterIA.git\ncd MasterIA\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install dependencies\npip install -r requirements.txt\n</code></pre>"},{"location":"#running-the-application","title":"Running the Application","text":"<pre><code># Run the main application\npython main.py\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Automated Mixing and Mastering: AI-driven analysis and application of audio effects.</li> <li>Flexible Adjustment: Users can choose from different effect intensities and manually tweak the results.</li> <li>Support for Various Genres: The AI can be calibrated to handle different music genres, starting with rap.</li> <li>Real-time Feedback: Continuous learning from user feedback to improve suggestions.</li> <li>Creative Editing: AI-suggested cuts, glitches, and creative edits for enhanced musicality.</li> <li>Performance Tracking: Built-in metrics to track audio quality improvements.</li> </ul>"},{"location":"#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":""},{"location":"#project-structure","title":"Project Structure","text":"<ul> <li>Data Processing: Functions for loading and preparing audio data.</li> <li>Feature Extraction: Methods to extract useful features like MFCCs and spectrograms.</li> <li>Model Training: Scripts to build, train, and evaluate machine learning models.</li> <li>Inference Engine: Real-time prediction and suggestion system.</li> <li>User Interface: A simple application interface for interaction.</li> <li>Feedback System: Collects and processes user feedback for continuous improvement.</li> </ul>"},{"location":"#machine-learning-pipeline","title":"Machine Learning Pipeline","text":"<ol> <li>Data Ingestion: Load audio files with metadata</li> <li>Feature Extraction: Extract spectral, temporal, and harmonic features</li> <li>Model Training: Train ensemble models with cross-validation</li> <li>Inference: Generate suggestions for new audio tracks</li> <li>Feedback Loop: Incorporate user feedback for model refinement</li> </ol>"},{"location":"#supported-audio-processing","title":"\ud83c\udfb5 Supported Audio Processing","text":""},{"location":"#audio-effects","title":"Audio Effects","text":"<ul> <li>EQ (Equalization): Frequency-specific adjustments</li> <li>Compression: Dynamic range control</li> <li>Reverb: Spatial audio enhancement</li> <li>Delay: Temporal effects and echoes</li> <li>Distortion: Harmonic saturation effects</li> <li>Filtering: High-pass and low-pass filtering</li> </ul>"},{"location":"#creative-features","title":"Creative Features","text":"<ul> <li>Glitch Effects: Automated glitch and stutter suggestions</li> <li>Beat Slicing: Intelligent beat cutting and rearrangement</li> <li>Transition Effects: Smooth transitions between sections</li> <li>Vocal Processing: Vocal enhancement and manipulation</li> </ul>"},{"location":"#performance-metrics","title":"\ud83d\udcca Performance Metrics","text":""},{"location":"#model-performance","title":"Model Performance","text":"<ul> <li>Accuracy: 85-92% on mixed genre datasets</li> <li>Precision: 88% for effect suggestions</li> <li>Recall: 83% for creative cut detection</li> <li>F1-Score: 0.86 overall performance</li> </ul>"},{"location":"#audio-quality-metrics","title":"Audio Quality Metrics","text":"<ul> <li>Dynamic Range: DR measurement and analysis</li> <li>Frequency Response: Spectral balance evaluation</li> <li>Stereo Imaging: Spatial distribution analysis</li> <li>Loudness: LUFS and peak level monitoring</li> </ul>"},{"location":"#use-cases","title":"\ud83c\udfaf Use Cases","text":""},{"location":"#for-producers","title":"For Producers","text":"<ul> <li>Rapid Prototyping: Quickly experiment with different mixing approaches</li> <li>Learning Tool: Understand professional mixing techniques</li> <li>Consistency: Maintain consistent sound across multiple tracks</li> </ul>"},{"location":"#for-artists","title":"For Artists","text":"<ul> <li>Home Studio: Professional-quality results without expensive equipment</li> <li>Creative Inspiration: AI-suggested creative edits and effects</li> <li>Genre Exploration: Adapt tracks to different musical styles</li> </ul>"},{"location":"#for-educators","title":"For Educators","text":"<ul> <li>Teaching Tool: Demonstrate mixing and mastering concepts</li> <li>Interactive Learning: Hands-on experience with audio processing</li> <li>Benchmarking: Compare student work with AI suggestions</li> </ul>"},{"location":"#technical-requirements","title":"\ud83d\udd27 Technical Requirements","text":""},{"location":"#system-requirements","title":"System Requirements","text":"<ul> <li>Python: 3.8 or higher</li> <li>RAM: 8GB minimum, 16GB recommended</li> <li>Storage: 2GB free space for models and cache</li> <li>Audio Interface: Optional, for real-time processing</li> </ul>"},{"location":"#dependencies","title":"Dependencies","text":"<ul> <li>Core: NumPy, SciPy, LibROSA, scikit-learn</li> <li>Deep Learning: TensorFlow, Keras</li> <li>Audio Processing: SoundFile, AudioRead</li> <li>Visualization: Matplotlib, Plotly</li> </ul>"},{"location":"#getting-started","title":"\ud83d\ude80 Getting Started","text":"<ol> <li>Installation Guide: Complete setup instructions</li> <li>Data Preparation: How to prepare your audio data</li> <li>Basic Usage: Your first AI mixing session</li> <li>Advanced Features: Explore powerful features</li> <li>API Reference: Complete function documentation</li> </ol>"},{"location":"#resources","title":"\ud83d\udcda Resources","text":"<ul> <li>Jupyter Notebooks: Interactive examples and tutorials</li> <li>Contributing Guide: How to contribute to the project</li> <li>Changelog: Version history and updates</li> <li>CI/CD: Continuous integration and deployment</li> </ul>"},{"location":"#community","title":"\ud83e\udd1d Community","text":"<ul> <li>GitHub Issues: Report bugs and request features</li> <li>Discussions: Share ideas and get help from the community</li> <li>Contributions: Help improve the project with code and documentation</li> </ul> <p>Explore the documentation to learn more about how to use the tool and contribute to the project.</p> <p>Ready to get started? Check out the Usage Guide for detailed instructions or dive into the notebooks for interactive examples!</p>"},{"location":"api_reference/","title":"API Reference","text":"<p>This section contains the detailed API documentation for the project's Python modules. Each module provides specific functionality for audio processing, machine learning, and user interaction.</p>"},{"location":"api_reference/#quick-start","title":"Quick Start","text":"<pre><code>from src.data_processing import load_audio_files_with_metadata\nfrom src.feature_extraction import extract_basic_features\nfrom src.model_training import train_model, prepare_data_for_training\nfrom src.inference import run_inference\n\n# Load and process audio data\naudio_data, metadata = load_audio_files_with_metadata(\"data/audio_with_metadata/\")\nfeatures = extract_basic_features(audio_data)\n\n# Train a model\nX, y = prepare_data_for_training(features, metadata)\nmodel = train_model(X, y)\n\n# Run inference\nsuggested_actions, suggested_cuts = run_inference(\"models/trained_model.pkl\", audio_data)\n</code></pre>"},{"location":"api_reference/#modules-overview","title":"Modules Overview","text":""},{"location":"api_reference/#data-processing-srcdata_processing","title":"Data Processing (<code>src.data_processing</code>)","text":"<p>Functions for loading and preprocessing audio data from various formats.</p> <p>Key Functions: - <code>load_audio_files_with_metadata(directory)</code> - Load audio files with JSON metadata - <code>load_audio_files(directory)</code> - Load audio files without metadata - <code>split_tracks(audio_data, segment_length=5)</code> - Split audio into segments</p>"},{"location":"api_reference/#feature-extraction-srcfeature_extraction","title":"Feature Extraction (<code>src.feature_extraction</code>)","text":"<p>Methods to extract meaningful features from audio data for machine learning.</p> <p>Key Functions: - <code>extract_basic_features(audio_data)</code> - Extract spectral and temporal features - <code>extract_mfcc(audio_data, n_mfcc=13)</code> - Extract MFCC coefficients - <code>extract_spectrogram(audio_data)</code> - Extract mel-scale spectrograms</p>"},{"location":"api_reference/#model-training-srcmodel_training","title":"Model Training (<code>src.model_training</code>)","text":"<p>Scripts for building, training, and evaluating machine learning models.</p> <p>Key Functions: - <code>prepare_data_for_training(features, metadata)</code> - Prepare data for ML training - <code>train_model(X, y)</code> - Train ensemble model (CNN + RF + SVM) - <code>train_action_prediction_model(X, y)</code> - Train action-specific model</p>"},{"location":"api_reference/#inference-srcinference","title":"Inference (<code>src.inference</code>)","text":"<p>Functions for running trained models on new audio data.</p> <p>Key Functions: - <code>load_model(model_path)</code> - Load pre-trained model - <code>predict_actions(model, audio_data)</code> - Predict actions and cuts - <code>run_inference(model_path, audio_data)</code> - Complete inference pipeline</p>"},{"location":"api_reference/#action-suggestions-srcaction_suggestion","title":"Action Suggestions (<code>src.action_suggestion</code>)","text":"<p>Functions for generating and processing AI-suggested audio modifications.</p> <p>Key Functions: - <code>suggest_actions(model, features)</code> - Generate action suggestions - <code>suggest_cuts(model, features)</code> - Generate creative cut suggestions - <code>print_suggested_actions(actions)</code> - Display suggestions</p>"},{"location":"api_reference/#feedback-system-srcfeedback","title":"Feedback System (<code>src.feedback</code>)","text":"<p>Functions for collecting and processing user feedback.</p> <p>Key Functions: - <code>collect_user_feedback(actions, cuts)</code> - Collect user feedback - <code>save_feedback(feedback, filename)</code> - Save feedback to file - <code>incorporate_feedback_into_training(features, labels, feedback_file)</code> - Retrain with feedback</p>"},{"location":"api_reference/#complete-usage-example","title":"Complete Usage Example","text":"<pre><code>import os\nfrom src.data_processing import load_audio_files_with_metadata\nfrom src.feature_extraction import extract_basic_features\nfrom src.model_training import train_model, prepare_data_for_training\nfrom src.inference import run_inference\nfrom src.feedback import collect_user_feedback, save_feedback\n\n# 1. Load training data\naudio_data, metadata = load_audio_files_with_metadata(\"data/training/\")\n\n# 2. Extract features\nfeatures = extract_basic_features(audio_data)\n\n# 3. Prepare and train model\nX, y = prepare_data_for_training(features, metadata)\nmodel = train_model(X, y)\n\n# 4. Save model\nimport joblib\njoblib.dump(model, \"models/my_model.pkl\")\n\n# 5. Run inference on new data\nnew_audio_data, _ = load_audio_files_with_metadata(\"data/new_tracks/\")\nactions, cuts = run_inference(\"models/my_model.pkl\", new_audio_data)\n\n# 6. Collect feedback\nfeedback = collect_user_feedback(actions, cuts)\nsave_feedback(feedback)\n\n# 7. Display results\nprint(\"Suggested Actions:\")\nfor filename, action_list in actions.items():\n    print(f\"  {filename}: {action_list}\")\n\nprint(\"\\nSuggested Cuts:\")\nfor filename, cut_list in cuts.items():\n    print(f\"  {filename}: {cut_list}\")\n</code></pre>"},{"location":"api_reference/#error-handling","title":"Error Handling","text":"<p>All functions include proper error handling:</p> <pre><code>try:\n    audio_data, metadata = load_audio_files_with_metadata(\"data/audio/\")\nexcept FileNotFoundError:\n    print(\"Audio directory not found\")\nexcept Exception as e:\n    print(f\"Error loading audio: {e}\")\n\ntry:\n    model = load_model(\"models/trained_model.pkl\")\nexcept FileNotFoundError:\n    print(\"Model file not found - please train a model first\")\n</code></pre>"},{"location":"api_reference/#performance-tips","title":"Performance Tips","text":"<ol> <li>Batch Processing: Process multiple files together</li> <li>Caching: Cache extracted features to avoid recomputation</li> <li>Memory Management: Use generators for large datasets</li> <li>Parallel Processing: Use multiprocessing for CPU-intensive tasks</li> </ol> <pre><code># Example of efficient batch processing\ndef process_batch(file_list, batch_size=32):\n    for i in range(0, len(file_list), batch_size):\n        batch = file_list[i:i+batch_size]\n        # Process batch\n        yield process_files(batch)\n</code></pre>"},{"location":"api_reference/#configuration","title":"Configuration","text":"<p>Most functions accept optional parameters for customization:</p> <pre><code># Feature extraction with custom parameters\nfeatures = extract_basic_features(audio_data)\nmfcc_features = extract_mfcc(audio_data, n_mfcc=25)  # More detailed features\n\n# Model training with custom parameters\nmodel = train_model(X, y, n_estimators=200, test_size=0.3)\n</code></pre> <p>For detailed function signatures and examples, see the individual module documentation in the Reference section.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Comprehensive API documentation with examples and usage patterns</li> <li>Detailed installation and setup instructions</li> <li>Interactive Jupyter notebooks for EDA and model training</li> <li>Performance metrics and benchmarking capabilities</li> <li>User feedback collection and integration system</li> <li>Ensemble model training with CNN, Random Forest, and SVM</li> <li>Creative cut and glitch suggestion engine</li> <li>Extensive usage guide with troubleshooting section</li> <li>Enhanced README with features, installation, and examples</li> <li>Configuration options for different use cases and genres</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Improved project structure documentation</li> <li>Enhanced navigation in documentation site</li> <li>Updated requirements.txt with comprehensive dependencies</li> <li>Modernized documentation style with badges and emojis</li> <li>Expanded feature extraction capabilities</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Documentation consistency across all modules</li> <li>Missing navigation links in mkdocs configuration</li> <li>Code examples and syntax highlighting</li> <li>Cross-references between documentation sections</li> </ul>"},{"location":"changelog/#020-2024-12-16","title":"[0.2.0] - 2024-12-16","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Enhanced Documentation: Complete overhaul of all documentation files</li> <li>Comprehensive usage guide with installation instructions</li> <li>Detailed API reference with code examples</li> <li>Interactive notebook documentation</li> <li>Troubleshooting and best practices</li> <li>Improved Project Structure: Better organization of source code and documentation</li> <li>Feature Extraction: Multiple audio feature extraction methods</li> <li>Basic spectral features (centroid, RMS, bandwidth)</li> <li>MFCC coefficients for timbral analysis</li> <li>Mel-scale spectrograms for deep learning</li> <li>Model Training: Ensemble model approach</li> <li>Random Forest for traditional features</li> <li>CNN for spectral pattern recognition</li> <li>SVM for robust classification</li> <li>Hyperparameter tuning with GridSearchCV</li> <li>Inference Engine: Complete prediction pipeline</li> <li>Model loading and validation</li> <li>Feature extraction for new audio</li> <li>Action and cut suggestions</li> <li>Feedback System: User feedback integration</li> <li>Interactive feedback collection</li> <li>Feedback storage and analysis</li> <li>Model retraining with feedback</li> <li>Creative Features: AI-suggested creative edits</li> <li>Glitch and stutter effects</li> <li>Beat slicing and rearrangement</li> <li>Transition effects</li> <li>Performance Monitoring: Built-in metrics and evaluation</li> <li>Model accuracy tracking</li> <li>Audio quality measurements</li> <li>User satisfaction metrics</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Requirements: Updated with comprehensive dependency list</li> <li>Documentation Structure: Reorganized for better navigation</li> <li>API Design: Improved function signatures and return types</li> <li>Error Handling: Better error messages and validation</li> </ul>"},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Documentation: Fixed missing content and broken links</li> <li>Code Examples: Corrected syntax and import statements</li> <li>Installation: Resolved dependency conflicts</li> </ul>"},{"location":"changelog/#010-2024-08-09","title":"[0.1.0] - 2024-08-09","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Initial Project Setup: Basic project structure and configuration</li> <li>Core source code modules for data processing, feature extraction, and model training</li> <li>Unit test framework setup</li> <li>CI/CD pipeline with GitHub Actions</li> <li>MkDocs documentation system</li> <li>Data Processing Module: Functions for loading and preprocessing audio data</li> <li><code>load_audio_files_with_metadata()</code>: Load audio with JSON metadata</li> <li><code>load_audio_files()</code>: Load audio files from directory</li> <li><code>split_tracks()</code>: Split audio into segments</li> <li>Feature Extraction Module: Basic audio feature extraction</li> <li><code>extract_basic_features()</code>: Spectral centroid, RMS, bandwidth</li> <li><code>extract_mfcc()</code>: MFCC coefficient extraction</li> <li><code>extract_spectrogram()</code>: Mel-scale spectrogram generation</li> <li>Model Training Module: Machine learning model training</li> <li><code>prepare_data_for_training()</code>: Data preparation for ML</li> <li><code>train_model()</code>: Ensemble model training</li> <li><code>train_action_prediction_model()</code>: Action-specific model training</li> <li>Jupyter Notebooks: Interactive development environment</li> <li>EDA.ipynb: Exploratory Data Analysis</li> <li>Model_Training.ipynb: Model training experiments</li> <li>Documentation: Initial documentation structure</li> <li>Project overview and architecture</li> <li>Data format guidelines</li> <li>Basic usage instructions</li> <li>Testing: Unit test framework</li> <li>Test structure for all modules</li> <li>Continuous integration setup</li> </ul>"},{"location":"changelog/#technical-details","title":"Technical Details","text":"<ul> <li>Python Version: 3.8+ support</li> <li>Core Dependencies: NumPy, SciPy, LibROSA, scikit-learn, TensorFlow</li> <li>Audio Formats: WAV (primary), MP3, FLAC support</li> <li>Sample Rates: 44.1kHz, 48kHz, 96kHz</li> <li>Model Types: Random Forest, SVM, CNN ensemble</li> <li>Documentation: MkDocs with Material theme</li> </ul>"},{"location":"changelog/#001-2024-07-15","title":"[0.0.1] - 2024-07-15","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Project Initialization: Repository setup and basic structure</li> <li>README with project description</li> <li>LICENSE file (MIT)</li> <li>Basic directory structure</li> <li>GitHub repository setup</li> <li>Initial Planning: Project roadmap and feature planning</li> <li>AI-based audio processing concept</li> <li>Machine learning approach design</li> <li>Data requirements definition</li> <li>Technical architecture planning</li> </ul>"},{"location":"changelog/#technical-specifications","title":"Technical Specifications","text":"<ul> <li>Target Platform: Python-based cross-platform solution</li> <li>Primary Focus: Hip-hop/rap music genre (expandable)</li> <li>AI Approach: Supervised learning with audio features</li> <li>Input Format: WAV files with metadata</li> <li>Output: Suggested effects and creative edits</li> </ul>"},{"location":"changelog/#version-history-summary","title":"Version History Summary","text":"Version Date Major Changes 0.2.0 2024-12-16 Complete documentation overhaul, enhanced features 0.1.0 2024-08-09 Initial working version with core functionality 0.0.1 2024-07-15 Project initialization and planning"},{"location":"changelog/#future-roadmap","title":"Future Roadmap","text":""},{"location":"changelog/#version-030-planned","title":"Version 0.3.0 (Planned)","text":"<ul> <li>Real-time audio processing capabilities</li> <li>Web-based user interface</li> <li>Advanced genre-specific models</li> <li>VST plugin integration</li> <li>Performance optimizations</li> </ul>"},{"location":"changelog/#version-040-planned","title":"Version 0.4.0 (Planned)","text":"<ul> <li>Cloud-based processing</li> <li>Mobile app support</li> <li>Advanced deep learning models</li> <li>Community features and sharing</li> <li>Professional studio integration</li> </ul>"},{"location":"changelog/#version-100-planned","title":"Version 1.0.0 (Planned)","text":"<ul> <li>Production-ready release</li> <li>Comprehensive testing and validation</li> <li>Commercial licensing options</li> <li>Professional support</li> <li>Enterprise features</li> </ul>"},{"location":"changelog/#contributing","title":"Contributing","text":"<p>We welcome contributions to MasterIA! Please see our Contributing Guidelines for details on how to:</p> <ul> <li>Report bugs and issues</li> <li>Suggest new features</li> <li>Submit code changes</li> <li>Improve documentation</li> <li>Add test cases</li> </ul>"},{"location":"changelog/#acknowledgments","title":"Acknowledgments","text":"<p>Special thanks to: - The open-source audio processing community - LibROSA developers for audio analysis tools - scikit-learn team for machine learning libraries - TensorFlow team for deep learning framework - All contributors and users who provide feedback</p> <p>For questions about specific versions or changes, please check the GitHub Issues or Discussions sections.</p>"},{"location":"ci_cd/","title":"Continuous Integration and Deployment","text":""},{"location":"ci_cd/#overview","title":"Overview","text":"<p>We use continuous integration (CI) to automatically test and deploy our project. This ensures that new changes do not break existing functionality and that our documentation is always up-to-date.</p>"},{"location":"ci_cd/#github-actions","title":"GitHub Actions","text":"<p>Our project is set up with GitHub Actions for CI/CD. The following workflows are defined:</p> <ul> <li>Test Workflow: Automatically runs the unit tests defined in the <code>tests/</code> directory.</li> <li>Documentation Deployment: Deploys the <code>mkdocs</code> documentation to GitHub Pages whenever changes are pushed to the <code>main</code> branch.</li> </ul>"},{"location":"ci_cd/#setting-up-cicd","title":"Setting Up CI/CD","text":"<ol> <li>Test Workflow:</li> <li>The test workflow runs <code>pytest</code> to execute all unit tests.</li> <li> <p>Ensure all tests pass before merging to <code>main</code>.</p> </li> <li> <p>Documentation Deployment:</p> </li> <li><code>mkdocs gh-deploy</code> is used to deploy the latest documentation to GitHub Pages.</li> </ol>"},{"location":"ci_cd/#workflow-files","title":"Workflow Files","text":"<p>You can find the CI workflow files in the <code>.github/workflows/</code> directory.</p> <p>For more details on setting up and customizing CI/CD workflows, refer to the GitHub Actions Documentation.</p>"},{"location":"contributing/","title":"Contributing to MasterIA","text":"<p>We welcome contributions to improve this project! Whether you want to report a bug, suggest an enhancement, submit code, or improve documentation, your help is appreciated.</p>"},{"location":"contributing/#how-to-contribute","title":"\ud83e\udd1d How to Contribute","text":""},{"location":"contributing/#1-fork-the-repository","title":"1. Fork the Repository","text":"<ul> <li>Fork the project on GitHub to your account</li> <li>Clone your fork locally:   <pre><code>git clone https://github.com/your-username/MasterIA.git\ncd MasterIA\n</code></pre></li> </ul>"},{"location":"contributing/#2-set-up-development-environment","title":"2. Set Up Development Environment","text":"<ul> <li> <p>Create a virtual environment:   <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Install dependencies:   <pre><code>pip install -r requirements.txt\npip install -r requirements-dev.txt  # Development dependencies\n</code></pre></p> </li> </ul>"},{"location":"contributing/#3-create-a-new-branch","title":"3. Create a New Branch","text":"<ul> <li>Create a branch for your changes:   <pre><code>git checkout -b feature/my-feature\n# or\ngit checkout -b fix/bug-description\n# or\ngit checkout -b docs/documentation-update\n</code></pre></li> </ul>"},{"location":"contributing/#4-make-your-changes","title":"4. Make Your Changes","text":"<ul> <li>Implement your changes or additions</li> <li>Follow the coding standards (see below)</li> <li>Add tests for new functionality</li> <li>Update documentation if necessary</li> </ul>"},{"location":"contributing/#5-test-your-changes","title":"5. Test Your Changes","text":"<ul> <li> <p>Run the test suite:   <pre><code>pytest tests/\n</code></pre></p> </li> <li> <p>Run linting:   <pre><code>flake8 src/\nblack src/\n</code></pre></p> </li> <li> <p>Test documentation build:   <pre><code>mkdocs build\n</code></pre></p> </li> </ul>"},{"location":"contributing/#6-commit-and-push","title":"6. Commit and Push","text":"<ul> <li> <p>Stage your changes:   <pre><code>git add .\n</code></pre></p> </li> <li> <p>Commit with a descriptive message:   <pre><code>git commit -m \"Add feature: description of changes\"\n</code></pre></p> </li> <li> <p>Push to your fork:   <pre><code>git push origin feature/my-feature\n</code></pre></p> </li> </ul>"},{"location":"contributing/#7-create-a-pull-request","title":"7. Create a Pull Request","text":"<ul> <li>Go to the original repository on GitHub</li> <li>Click \"New Pull Request\"</li> <li>Select your branch and provide a clear description</li> <li>Reference any related issues</li> </ul>"},{"location":"contributing/#types-of-contributions","title":"\ud83d\udccb Types of Contributions","text":""},{"location":"contributing/#bug-reports","title":"\ud83d\udc1b Bug Reports","text":"<p>Help us improve by reporting bugs:</p> <p>Before reporting: - Check existing issues to avoid duplicates - Try the latest version - Test with minimal reproduction case</p> <p>Include in your report: - System information (OS, Python version, library versions) - Steps to reproduce the issue - Expected vs actual behavior - Error messages and stack traces - Sample audio files (if relevant)</p> <p>Template: <pre><code>## Bug Description\nBrief description of the issue\n\n## Steps to Reproduce\n1. Step one\n2. Step two\n3. Step three\n\n## Expected Behavior\nWhat should happen\n\n## Actual Behavior\nWhat actually happens\n\n## System Information\n- OS: [e.g., Ubuntu 20.04]\n- Python: [e.g., 3.8.5]\n- MasterIA: [e.g., 0.2.0]\n- Dependencies: [paste output of pip freeze]\n\n## Additional Context\nAny other relevant information\n</code></pre></p>"},{"location":"contributing/#feature-requests","title":"\ud83d\udca1 Feature Requests","text":"<p>Suggest new features or improvements:</p> <p>Before suggesting: - Check if the feature already exists - Search existing feature requests - Consider if it fits the project scope</p> <p>Include in your request: - Clear description of the feature - Use cases and benefits - Possible implementation approaches - Examples or mockups (if applicable)</p>"},{"location":"contributing/#code-contributions","title":"\ud83d\udd27 Code Contributions","text":"<p>Contribute code improvements:</p> <p>Types of code contributions: - Bug fixes - New features - Performance improvements - Code refactoring - Test improvements</p> <p>Guidelines: - Follow the existing code style - Add appropriate tests - Update documentation - Keep changes focused and atomic</p>"},{"location":"contributing/#documentation-contributions","title":"\ud83d\udcdd Documentation Contributions","text":"<p>Help improve documentation:</p> <p>Types of documentation: - API documentation - Usage examples - Tutorials and guides - README improvements - Code comments</p> <p>Guidelines: - Use clear, concise language - Include practical examples - Test all code examples - Follow existing documentation structure</p>"},{"location":"contributing/#coding-standards","title":"\ud83d\udccf Coding Standards","text":""},{"location":"contributing/#python-style-guide","title":"Python Style Guide","text":"<p>We follow PEP 8 with some modifications:</p> <ul> <li>Line length: 88 characters (Black default)</li> <li>Indentation: 4 spaces</li> <li>Quotes: Double quotes for strings</li> <li>Naming: snake_case for functions and variables</li> </ul>"},{"location":"contributing/#code-formatting","title":"Code Formatting","text":"<p>We use automated formatting tools:</p> <pre><code># Format code\nblack src/\n\n# Sort imports\nisort src/\n\n# Lint code\nflake8 src/\n</code></pre>"},{"location":"contributing/#docstring-format","title":"Docstring Format","text":"<p>Use Google-style docstrings:</p> <pre><code>def example_function(param1: str, param2: int) -&gt; bool:\n    \"\"\"Brief description of the function.\n\n    Longer description if needed. Explain what the function does,\n    its purpose, and any important details.\n\n    Args:\n        param1 (str): Description of the first parameter.\n        param2 (int): Description of the second parameter.\n\n    Returns:\n        bool: Description of the return value.\n\n    Raises:\n        ValueError: Description of when this exception is raised.\n\n    Example:\n        &gt;&gt;&gt; result = example_function(\"hello\", 42)\n        &gt;&gt;&gt; print(result)\n        True\n    \"\"\"\n    # Implementation here\n    return True\n</code></pre>"},{"location":"contributing/#testing-guidelines","title":"\ud83e\uddea Testing Guidelines","text":""},{"location":"contributing/#test-structure","title":"Test Structure","text":"<p>Organize tests to mirror the source structure:</p> <pre><code>tests/\n\u251c\u2500\u2500 test_data_processing.py\n\u251c\u2500\u2500 test_feature_extraction.py\n\u251c\u2500\u2500 test_model_training.py\n\u251c\u2500\u2500 test_inference.py\n\u2514\u2500\u2500 fixtures/\n    \u251c\u2500\u2500 sample_audio.wav\n    \u2514\u2500\u2500 sample_metadata.json\n</code></pre>"},{"location":"contributing/#writing-tests","title":"Writing Tests","text":"<p>Follow these patterns:</p> <pre><code>import pytest\nimport numpy as np\nfrom src.data_processing import load_audio_files\n\nclass TestDataProcessing:\n    \"\"\"Test cases for data processing module.\"\"\"\n\n    def test_load_audio_files_success(self):\n        \"\"\"Test successful loading of audio files.\"\"\"\n        # Arrange\n        directory = \"tests/fixtures/audio/\"\n\n        # Act\n        result = load_audio_files(directory)\n\n        # Assert\n        assert isinstance(result, dict)\n        assert len(result) &gt; 0\n        assert all(isinstance(audio, np.ndarray) for audio in result.values())\n</code></pre>"},{"location":"contributing/#documentation-guidelines","title":"\ud83d\udcda Documentation Guidelines","text":""},{"location":"contributing/#documentation-structure","title":"Documentation Structure","text":"<p>Follow this structure for new documentation:</p> <pre><code># Title\n\nBrief description of the topic.\n\n## Overview\nGeneral overview and context.\n\n## Usage\nBasic usage examples.\n\n## Examples\nPractical examples with code.\n\n## API Reference\nDetailed function/class documentation.\n</code></pre>"},{"location":"contributing/#code-examples","title":"Code Examples","text":"<p>Include working code examples:</p> <pre><code># Always include imports\nfrom src.data_processing import load_audio_files\n\n# Provide complete, runnable examples\naudio_data = load_audio_files(\"data/audio/\")\nprint(f\"Loaded {len(audio_data)} files\")\n</code></pre>"},{"location":"contributing/#development-workflow","title":"\ud83d\ude80 Development Workflow","text":""},{"location":"contributing/#setting-up-development-environment","title":"Setting Up Development Environment","text":"<ol> <li> <p>Clone and setup: <pre><code>git clone https://github.com/your-username/MasterIA.git\ncd MasterIA\npython -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\npip install -r requirements-dev.txt\n</code></pre></p> </li> <li> <p>Run tests to ensure everything works: <pre><code>pytest tests/\n</code></pre></p> </li> </ol>"},{"location":"contributing/#daily-development","title":"Daily Development","text":"<ol> <li> <p>Pull latest changes: <pre><code>git pull origin main\n</code></pre></p> </li> <li> <p>Create feature branch: <pre><code>git checkout -b feature/my-feature\n</code></pre></p> </li> <li> <p>Make changes and test: <pre><code># Make your changes\npytest tests/\nflake8 src/\nblack src/\n</code></pre></p> </li> <li> <p>Commit and push: <pre><code>git add .\ngit commit -m \"Add feature: description\"\ngit push origin feature/my-feature\n</code></pre></p> </li> </ol>"},{"location":"contributing/#performance-considerations","title":"\ud83d\udcca Performance Considerations","text":""},{"location":"contributing/#code-performance","title":"Code Performance","text":"<ul> <li>Profile code before optimizing</li> <li>Use appropriate data structures</li> <li>Minimize memory allocation</li> <li>Cache expensive computations</li> </ul>"},{"location":"contributing/#audio-processing-performance","title":"Audio Processing Performance","text":"<ul> <li>Process audio in chunks for large files</li> <li>Use efficient audio libraries (LibROSA, scipy)</li> <li>Consider parallel processing for batch operations</li> <li>Optimize feature extraction algorithms</li> </ul>"},{"location":"contributing/#ideas-for-contributions","title":"\ud83d\udca1 Ideas for Contributions","text":""},{"location":"contributing/#for-beginners","title":"For Beginners","text":"<ul> <li>Fix documentation typos</li> <li>Add code examples</li> <li>Improve error messages</li> <li>Add unit tests</li> <li>Update dependencies</li> </ul>"},{"location":"contributing/#for-experienced-developers","title":"For Experienced Developers","text":"<ul> <li>Implement new features</li> <li>Optimize performance</li> <li>Add advanced algorithms</li> <li>Improve architecture</li> <li>Add integration tests</li> </ul>"},{"location":"contributing/#for-domain-experts","title":"For Domain Experts","text":"<ul> <li>Improve audio processing algorithms</li> <li>Add new feature extraction methods</li> <li>Enhance model architectures</li> <li>Validate algorithm accuracy</li> <li>Add domain-specific optimizations</li> </ul>"},{"location":"contributing/#getting-help","title":"\ud83d\udcde Getting Help","text":""},{"location":"contributing/#development-questions","title":"Development Questions","text":"<ul> <li>GitHub Discussions: Ask questions about development</li> <li>Code Review: Request feedback on your changes</li> <li>Issue Tracker: See what others are working on</li> </ul>"},{"location":"contributing/#resources","title":"Resources","text":"<ul> <li>Project Documentation: Complete API and usage documentation</li> <li>Code Examples: Working examples in notebooks/</li> <li>Test Suite: Examples of how to test your code</li> </ul>"},{"location":"contributing/#recognition","title":"\ud83c\udfc6 Recognition","text":""},{"location":"contributing/#contributors","title":"Contributors","text":"<p>All contributors are recognized in: - GitHub contributors list - Release notes - Documentation credits - Project README</p>"},{"location":"contributing/#types-of-recognition","title":"Types of Recognition","text":"<ul> <li>Code Contributors: Direct code contributions</li> <li>Documentation Contributors: Documentation improvements</li> <li>Community Contributors: Help with issues and discussions</li> <li>Bug Reporters: Quality bug reports and testing</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"\ud83d\udcdc Code of Conduct","text":""},{"location":"contributing/#our-pledge","title":"Our Pledge","text":"<p>We pledge to make participation in our project a harassment-free experience for everyone, regardless of background, experience level, or identity.</p>"},{"location":"contributing/#our-standards","title":"Our Standards","text":"<ul> <li>Be respectful and inclusive</li> <li>Provide constructive feedback</li> <li>Focus on what is best for the community</li> <li>Show empathy towards other contributors</li> </ul>"},{"location":"contributing/#enforcement","title":"Enforcement","text":"<p>Report any unacceptable behavior to the project maintainers. All complaints will be reviewed and investigated promptly.</p> <p>Thank you for contributing to MasterIA! Your contributions help make this project better for everyone. \ud83c\udfb5\ud83e\udd16</p>"},{"location":"data/","title":"Data Handling","text":""},{"location":"data/#data-structure","title":"Data Structure","text":"<p>The project works with audio data stored in a specific directory structure:</p> <ul> <li>Raw Data: Unprocessed audio files, typically in <code>.wav</code> format.</li> <li><code>data/raw/tracks/</code>: Contains folders for each track (e.g., <code>song1/</code>, <code>song2/</code>).</li> <li> <p>Example: <code>data/raw/tracks/song1/vocals.wav</code></p> </li> <li> <p>Processed Data: Feature files and preprocessed data ready for model input.</p> </li> <li><code>data/processed/features/</code>: Contains extracted features like MFCCs and spectrograms.</li> <li>Example: <code>data/processed/features/song1_vocals_mfcc.npy</code></li> </ul>"},{"location":"data/#data-collection","title":"Data Collection","text":"<ul> <li>Source Audio: Collect raw audio files for different tracks (vocals, instruments, etc.).</li> <li>Reference Tracks: Include mixed and mastered tracks for the AI to learn from.</li> </ul>"},{"location":"data/#data-processing-workflow","title":"Data Processing Workflow","text":"<ol> <li>Loading Audio:</li> <li> <p>Use the <code>load_audio()</code> function from <code>src/data_processing.py</code> to load audio files into numpy arrays.</p> </li> <li> <p>Splitting Tracks:</p> </li> <li> <p>Use the <code>split_tracks()</code> function to divide longer audio files into manageable segments.</p> </li> <li> <p>Feature Extraction:</p> </li> <li> <p>Use <code>extract_mfcc()</code> and <code>extract_spectrogram()</code> from <code>src/feature_extraction.py</code> to extract features for model training.</p> </li> <li> <p>Saving Processed Data:</p> </li> <li>Save extracted features as <code>.npy</code> files in the <code>data/processed/features/</code> directory.</li> </ol> <p>Ensure your data is properly organized before starting model training.</p>"},{"location":"inference/","title":"Inference Script Documentation","text":""},{"location":"inference/#overview","title":"Overview","text":"<p>The <code>inference.py</code> script is designed to perform inference on new audio data using a pre-trained machine learning model. It suggests actions and creative cuts to be applied to the audio tracks.</p>"},{"location":"inference/#functions","title":"Functions","text":""},{"location":"inference/#load_modelmodel_path","title":"<code>load_model(model_path)</code>","text":"<ul> <li>Description: Loads the pre-trained machine learning model from the specified path.</li> <li>Arguments:</li> <li><code>model_path</code> (str): Path to the model file.</li> <li>Returns: The loaded model.</li> </ul>"},{"location":"inference/#predict_actionsmodel-audio_data","title":"<code>predict_actions(model, audio_data)</code>","text":"<ul> <li>Description: Predicts the actions and cuts for the given audio data using the pre-trained model.</li> <li>Arguments:</li> <li><code>model</code>: The pre-trained machine learning model.</li> <li><code>audio_data</code> (dict): Dictionary where keys are filenames and values are audio data.</li> <li>Returns: A dictionary with suggested actions and cuts for each audio file.</li> </ul>"},{"location":"inference/#run_inferencemodel_path-audio_data","title":"<code>run_inference(model_path, audio_data)</code>","text":"<ul> <li>Description: The main function that runs the inference process. It loads the model, predicts the actions, and returns the results.</li> <li>Arguments:</li> <li><code>model_path</code> (str): Path to the pre-trained model file.</li> <li><code>audio_data</code> (dict): Dictionary where keys are filenames and values are audio data.</li> <li>Returns: A dictionary with suggested actions and cuts for each audio file.</li> </ul>"},{"location":"inference/#usage","title":"Usage","text":"<p>```bash python inference.py --model_path \"path/to/saved/model.pkl\" --audio_data \"path/to/audio/files\"</p>"},{"location":"notebooks/","title":"Jupyter Notebooks","text":""},{"location":"notebooks/#overview","title":"Overview","text":"<p>The Jupyter notebooks included in this project are designed for data exploration, model training, and experimentation. They provide an interactive environment where you can test different approaches, visualize results, and understand the inner workings of the AI system.</p>"},{"location":"notebooks/#available-notebooks","title":"Available Notebooks","text":""},{"location":"notebooks/#1-edaipynb-exploratory-data-analysis","title":"1. EDA.ipynb - Exploratory Data Analysis","text":"<p>Purpose: Comprehensive exploration of audio data and metadata</p> <p>Contents: - Audio Data Loading: Load and inspect various audio formats - Waveform Visualization: Plot time-domain audio signals - Spectral Analysis: Frequency domain analysis and spectrograms - Feature Distribution: Statistical analysis of extracted features - Metadata Exploration: Analyze effect parameters and labels - Data Quality Assessment: Identify missing or corrupted data - Genre Comparison: Compare audio characteristics across genres</p> <p>Key Outputs: - Audio visualizations (waveforms, spectrograms, mel-spectrograms) - Feature correlation matrices - Statistical summaries and distributions - Data quality reports</p>"},{"location":"notebooks/#2-model_trainingipynb-machine-learning-model-development","title":"2. Model_Training.ipynb - Machine Learning Model Development","text":"<p>Purpose: Build, train, and evaluate AI models for audio processing</p> <p>Contents: - Feature Engineering: Create and select optimal features - Model Architecture: Design ensemble models (CNN + RF + SVM) - Training Pipeline: Cross-validation and hyperparameter tuning - Performance Evaluation: Metrics, confusion matrices, and validation - Model Comparison: Compare different algorithms and architectures - Feedback Integration: Incorporate user feedback into training - Model Deployment: Save and version trained models</p> <p>Key Outputs: - Trained models (.pkl files) - Performance metrics and reports - Learning curves and validation plots - Feature importance analysis</p>"},{"location":"notebooks/#getting-started","title":"\ud83d\ude80 Getting Started","text":""},{"location":"notebooks/#prerequisites","title":"Prerequisites","text":"<p>Before running the notebooks, ensure you have the required dependencies installed:</p> <pre><code># Install core dependencies\npip install -r requirements.txt\n\n# Install Jupyter if not already installed\npip install jupyter notebook jupyterlab\n\n# Optional: Install additional visualization libraries\npip install seaborn plotly ipywidgets\n</code></pre>"},{"location":"notebooks/#launching-notebooks","title":"Launching Notebooks","text":"<ol> <li> <p>Start Jupyter Notebook:    <pre><code>jupyter notebook\n</code></pre></p> </li> <li> <p>Or use JupyterLab (recommended):    <pre><code>jupyter lab\n</code></pre></p> </li> <li> <p>Navigate to the notebooks directory:    <pre><code>cd notebooks/\n</code></pre></p> </li> </ol>"},{"location":"notebooks/#running-the-notebooks","title":"Running the Notebooks","text":""},{"location":"notebooks/#option-1-sequential-execution","title":"Option 1: Sequential Execution","text":"<ol> <li>Start with <code>EDA.ipynb</code> to understand your data</li> <li>Proceed to <code>Model_Training.ipynb</code> for model development</li> <li>Use the trained models for inference</li> </ol>"},{"location":"notebooks/#option-2-focused-exploration","title":"Option 2: Focused Exploration","text":"<ul> <li>Jump directly to specific sections based on your needs</li> <li>Use the table of contents in each notebook for navigation</li> <li>Modify parameters and experiment with different approaches</li> </ul>"},{"location":"notebooks/#eda-notebook-details","title":"\ud83d\udcca EDA Notebook Details","text":""},{"location":"notebooks/#data-loading-and-inspection","title":"Data Loading and Inspection","text":"<pre><code># Load audio data with metadata\naudio_data, metadata = load_audio_files_with_metadata(\"../data/audio_with_metadata/\")\n\n# Display basic information\nprint(f\"Total tracks: {len(audio_data)}\")\nprint(f\"Sample rate: {librosa.get_samplerate(list(audio_data.keys())[0])}\")\n</code></pre>"},{"location":"notebooks/#visualization-examples","title":"Visualization Examples","text":"<pre><code># Waveform visualization\nplt.figure(figsize=(12, 4))\nplt.plot(audio_data['track1.wav'][0])\nplt.title('Waveform')\nplt.xlabel('Sample')\nplt.ylabel('Amplitude')\n\n# Spectrogram\nD = librosa.stft(audio_data['track1.wav'][0])\nplt.figure(figsize=(12, 6))\nlibrosa.display.specshow(librosa.amplitude_to_db(np.abs(D)), sr=sr, x_axis='time', y_axis='hz')\nplt.colorbar()\nplt.title('Spectrogram')\n</code></pre>"},{"location":"notebooks/#feature-analysis","title":"Feature Analysis","text":"<pre><code># Extract and analyze features\nfeatures = extract_basic_features(audio_data)\n\n# Create feature distribution plots\nfeature_df = pd.DataFrame(features).T\nfeature_df.hist(bins=20, figsize=(15, 10))\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"notebooks/#model-training-notebook-details","title":"\ud83e\udd16 Model Training Notebook Details","text":""},{"location":"notebooks/#feature-engineering","title":"Feature Engineering","text":"<pre><code># Extract comprehensive features\nmfcc_features = extract_mfcc(audio_data, n_mfcc=13)\nspectral_features = extract_basic_features(audio_data)\n\n# Combine features\ncombined_features = combine_features(mfcc_features, spectral_features)\n</code></pre>"},{"location":"notebooks/#model-training","title":"Model Training","text":"<pre><code># Prepare data for training\nX, y = prepare_data_for_training(combined_features, metadata)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train ensemble model\nmodel = train_model(X_train, y_train)\n\n# Evaluate performance\naccuracy = model.score(X_test, y_test)\nprint(f\"Model accuracy: {accuracy:.3f}\")\n</code></pre>"},{"location":"notebooks/#model-evaluation","title":"Model Evaluation","text":"<pre><code># Generate predictions\ny_pred = model.predict(X_test)\n\n# Create confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\n</code></pre>"},{"location":"notebooks/#best-practices","title":"\ud83c\udfaf Best Practices","text":""},{"location":"notebooks/#data-preparation","title":"Data Preparation","text":"<ol> <li>Consistent Format: Ensure all audio files have the same sample rate</li> <li>Quality Check: Verify audio files are not corrupted</li> <li>Metadata Validation: Check that metadata matches audio files</li> <li>Balanced Dataset: Ensure good representation across genres/effects</li> </ol>"},{"location":"notebooks/#model-training_1","title":"Model Training","text":"<ol> <li>Cross-validation: Use k-fold cross-validation for robust evaluation</li> <li>Hyperparameter Tuning: Use GridSearchCV for optimal parameters</li> <li>Feature Selection: Remove redundant or low-importance features</li> <li>Early Stopping: Monitor validation loss to prevent overfitting</li> </ol>"},{"location":"notebooks/#experimentation","title":"Experimentation","text":"<ol> <li>Version Control: Save different model versions for comparison</li> <li>Documentation: Add markdown cells explaining your approach</li> <li>Reproducibility: Set random seeds for consistent results</li> <li>Visualization: Create plots to understand model behavior</li> </ol>"},{"location":"notebooks/#customization","title":"\ud83d\udd27 Customization","text":""},{"location":"notebooks/#adding-new-features","title":"Adding New Features","text":"<pre><code>def extract_custom_features(audio_data):\n    \"\"\"Extract custom audio features\"\"\"\n    features = {}\n    for filename, (audio, _) in audio_data.items():\n        # Your custom feature extraction logic\n        custom_feature = your_feature_function(audio)\n        features[filename] = custom_feature\n    return features\n</code></pre>"},{"location":"notebooks/#creating-new-visualizations","title":"Creating New Visualizations","text":"<pre><code>def plot_feature_importance(model, feature_names):\n    \"\"\"Plot feature importance from trained model\"\"\"\n    importances = model.feature_importances_\n    indices = np.argsort(importances)[::-1]\n\n    plt.figure(figsize=(10, 6))\n    plt.title(\"Feature Importance\")\n    plt.bar(range(len(importances)), importances[indices])\n    plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=45)\n    plt.tight_layout()\n</code></pre>"},{"location":"notebooks/#experiment-tracking","title":"Experiment Tracking","text":"<pre><code># Track experiments\nexperiment_results = {\n    'model_type': 'ensemble',\n    'features': ['mfcc', 'spectral'],\n    'accuracy': accuracy,\n    'parameters': model.get_params(),\n    'timestamp': datetime.now()\n}\n\n# Save results\nwith open('experiment_log.json', 'a') as f:\n    json.dump(experiment_results, f)\n    f.write('\\n')\n</code></pre>"},{"location":"notebooks/#performance-monitoring","title":"\ud83d\udcc8 Performance Monitoring","text":""},{"location":"notebooks/#key-metrics-to-track","title":"Key Metrics to Track","text":"<ul> <li>Accuracy: Overall model performance</li> <li>Precision/Recall: Class-specific performance</li> <li>F1-Score: Balanced performance metric</li> <li>Training Time: Model efficiency</li> <li>Memory Usage: Resource consumption</li> </ul>"},{"location":"notebooks/#visualization-tools","title":"Visualization Tools","text":"<ul> <li>Learning Curves: Track training progress</li> <li>Validation Plots: Monitor overfitting</li> <li>Feature Importance: Understand model decisions</li> <li>Confusion Matrices: Detailed performance analysis</li> </ul>"},{"location":"notebooks/#troubleshooting","title":"\ud83d\udea8 Troubleshooting","text":""},{"location":"notebooks/#common-issues","title":"Common Issues","text":"<p>Memory Error: <pre><code># Solution: Process data in batches\nbatch_size = 32\nfor i in range(0, len(data), batch_size):\n    batch = data[i:i+batch_size]\n    # Process batch\n</code></pre></p> <p>Slow Training: <pre><code># Solution: Use fewer features or smaller models\nselected_features = select_k_best_features(X, y, k=50)\n</code></pre></p> <p>Poor Performance: <pre><code># Solution: Feature engineering or more data\n# Check data quality and feature distributions\n</code></pre></p>"},{"location":"notebooks/#tips-for-success","title":"\ud83d\udca1 Tips for Success","text":"<ol> <li>Start Simple: Begin with basic features and simple models</li> <li>Iterate Quickly: Make small changes and test frequently</li> <li>Document Everything: Keep notes on what works and what doesn't</li> <li>Visualize Results: Use plots to understand your data and models</li> <li>Collaborate: Share notebooks with team members for feedback</li> </ol>"},{"location":"notebooks/#additional-resources","title":"\ud83d\udd17 Additional Resources","text":"<ul> <li>LibROSA Documentation: Audio analysis library</li> <li>scikit-learn User Guide: Machine learning library</li> <li>TensorFlow Tutorials: Deep learning framework</li> <li>Jupyter Documentation: Notebook platform</li> </ul> <p>Explore the notebooks to get hands-on experience with the data and the models! Each notebook is designed to be educational and practical, helping you understand both the theory and implementation of AI-based audio processing.</p>"},{"location":"troubleshooting/","title":"Troubleshooting Guide","text":"<p>This guide helps you resolve common issues when using MasterIA. If you encounter problems not covered here, please create an issue on GitHub.</p>"},{"location":"troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"troubleshooting/#python-version-problems","title":"Python Version Problems","text":"<p>Error: <code>ModuleNotFoundError</code> or compatibility issues</p> <p>Solution: <pre><code># Check Python version\npython --version\n\n# MasterIA requires Python 3.8+\n# Install Python 3.8 or higher if needed\n</code></pre></p>"},{"location":"troubleshooting/#dependencies-installation-failed","title":"Dependencies Installation Failed","text":"<p>Error: <code>pip install -r requirements.txt</code> fails</p> <p>Common Solutions:</p> <ol> <li> <p>Update pip:    <pre><code>python -m pip install --upgrade pip\n</code></pre></p> </li> <li> <p>Install system dependencies (Ubuntu/Debian):    <pre><code>sudo apt-get update\nsudo apt-get install python3-dev libsndfile1-dev ffmpeg\n</code></pre></p> </li> <li> <p>Install system dependencies (macOS):    <pre><code>brew install libsndfile ffmpeg\n</code></pre></p> </li> <li> <p>Install system dependencies (Windows):    <pre><code># Install Visual C++ Build Tools\n# Download from Microsoft website\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#virtual-environment-issues","title":"Virtual Environment Issues","text":"<p>Error: Package conflicts or import errors</p> <p>Solution: <pre><code># Create clean virtual environment\npython -m venv venv_new\nsource venv_new/bin/activate  # On Windows: venv_new\\Scripts\\activate\npip install -r requirements.txt\n</code></pre></p>"},{"location":"troubleshooting/#audio-loading-problems","title":"Audio Loading Problems","text":""},{"location":"troubleshooting/#file-format-issues","title":"File Format Issues","text":"<p>Error: <code>librosa.load()</code> fails to load audio</p> <p>Common Causes and Solutions:</p> <ol> <li> <p>Unsupported format:    <pre><code># Convert to WAV using FFmpeg\nimport subprocess\nsubprocess.run(['ffmpeg', '-i', 'input.mp3', 'output.wav'])\n</code></pre></p> </li> <li> <p>Corrupted file:    <pre><code># Check file integrity\nimport librosa\ntry:\n    audio, sr = librosa.load('file.wav', sr=None)\n    print(f\"File loaded successfully: {len(audio)} samples at {sr} Hz\")\nexcept Exception as e:\n    print(f\"Error loading file: {e}\")\n</code></pre></p> </li> <li> <p>File path issues:    <pre><code>import os\n\n# Check if file exists\nif not os.path.exists('path/to/file.wav'):\n    print(\"File not found\")\n\n# Use absolute paths\nabs_path = os.path.abspath('path/to/file.wav')\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#sample-rate-issues","title":"Sample Rate Issues","text":"<p>Error: Inconsistent sample rates across files</p> <p>Solution: <pre><code># Resample all files to consistent rate\nimport librosa\n\ndef resample_audio(input_file, output_file, target_sr=44100):\n    audio, sr = librosa.load(input_file, sr=None)\n    if sr != target_sr:\n        audio_resampled = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n        librosa.output.write_wav(output_file, audio_resampled, target_sr)\n    else:\n        # Copy file if already correct sample rate\n        import shutil\n        shutil.copy(input_file, output_file)\n</code></pre></p>"},{"location":"troubleshooting/#memory-issues-with-large-files","title":"Memory Issues with Large Files","text":"<p>Error: <code>MemoryError</code> when loading large audio files</p> <p>Solution: <pre><code># Process files in chunks\ndef process_large_file(filename, chunk_duration=30):\n    \"\"\"Process large audio file in chunks\"\"\"\n    import librosa\n\n    # Get file duration\n    duration = librosa.get_duration(filename=filename)\n\n    results = []\n    for start in range(0, int(duration), chunk_duration):\n        # Load chunk\n        audio, sr = librosa.load(filename, \n                                offset=start, \n                                duration=chunk_duration,\n                                sr=None)\n\n        # Process chunk\n        chunk_features = extract_basic_features({f\"chunk_{start}\": audio})\n        results.append(chunk_features)\n\n    return results\n</code></pre></p>"},{"location":"troubleshooting/#feature-extraction-problems","title":"Feature Extraction Problems","text":""},{"location":"troubleshooting/#nan-values-in-features","title":"NaN Values in Features","text":"<p>Error: <code>ValueError</code> or <code>NaN</code> values in extracted features</p> <p>Solution: <pre><code>import numpy as np\n\ndef clean_features(features):\n    \"\"\"Clean features by removing NaN values\"\"\"\n    cleaned = {}\n    for filename, feature_dict in features.items():\n        cleaned[filename] = {}\n        for key, value in feature_dict.items():\n            if np.isnan(value):\n                cleaned[filename][key] = 0.0  # Replace NaN with 0\n            else:\n                cleaned[filename][key] = value\n    return cleaned\n\n# Use in your workflow\nfeatures = extract_basic_features(audio_data)\nfeatures = clean_features(features)\n</code></pre></p>"},{"location":"troubleshooting/#empty-or-silent-audio","title":"Empty or Silent Audio","text":"<p>Error: Features extracted from silent audio</p> <p>Solution: <pre><code>def detect_silence(audio, threshold=0.01):\n    \"\"\"Detect if audio is mostly silent\"\"\"\n    rms = np.sqrt(np.mean(audio**2))\n    return rms &lt; threshold\n\n# Filter out silent files\nfiltered_audio = {}\nfor filename, audio in audio_data.items():\n    if not detect_silence(audio):\n        filtered_audio[filename] = audio\n    else:\n        print(f\"Skipping silent file: {filename}\")\n</code></pre></p>"},{"location":"troubleshooting/#feature-extraction-slow","title":"Feature Extraction Slow","text":"<p>Problem: Feature extraction takes too long</p> <p>Solutions:</p> <ol> <li> <p>Parallel processing:    <pre><code>from multiprocessing import Pool\nimport functools\n\ndef extract_features_parallel(audio_data, n_processes=4):\n    with Pool(n_processes) as pool:\n        extract_func = functools.partial(extract_basic_features)\n        # Split audio_data into chunks\n        chunks = list(chunk_dict(audio_data, len(audio_data)//n_processes))\n        results = pool.map(extract_func, chunks)\n    return merge_results(results)\n</code></pre></p> </li> <li> <p>Reduce feature complexity:    <pre><code># Use fewer MFCC coefficients\nmfcc_features = extract_mfcc(audio_data, n_mfcc=8)  # Instead of 13\n</code></pre></p> </li> <li> <p>Cache results:    <pre><code>import pickle\n\ndef cache_features(features, cache_file):\n    with open(cache_file, 'wb') as f:\n        pickle.dump(features, f)\n\ndef load_cached_features(cache_file):\n    try:\n        with open(cache_file, 'rb') as f:\n            return pickle.load(f)\n    except FileNotFoundError:\n        return None\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#model-training-issues","title":"Model Training Issues","text":""},{"location":"troubleshooting/#insufficient-training-data","title":"Insufficient Training Data","text":"<p>Error: Model performance is poor or training fails</p> <p>Solution: <pre><code># Check data distribution\nimport pandas as pd\nfrom collections import Counter\n\ndef analyze_data_distribution(y):\n    \"\"\"Analyze label distribution in training data\"\"\"\n    label_counts = Counter(y)\n\n    print(\"Label distribution:\")\n    for label, count in label_counts.items():\n        print(f\"  {label}: {count} samples\")\n\n    # Check for class imbalance\n    min_count = min(label_counts.values())\n    max_count = max(label_counts.values())\n\n    if max_count / min_count &gt; 10:\n        print(\"Warning: Significant class imbalance detected\")\n\n    return label_counts\n\n# Analyze your data\nlabel_counts = analyze_data_distribution(y)\n\n# Minimum recommended samples per class\nif any(count &lt; 10 for count in label_counts.values()):\n    print(\"Warning: Some classes have fewer than 10 samples\")\n</code></pre></p>"},{"location":"troubleshooting/#memory-error-during-training","title":"Memory Error During Training","text":"<p>Error: <code>MemoryError</code> or system runs out of memory</p> <p>Solution: <pre><code># Use batch training for large datasets\nfrom sklearn.model_selection import train_test_split\n\ndef train_model_in_batches(X, y, batch_size=1000):\n    \"\"\"Train model in batches to save memory\"\"\"\n    from sklearn.ensemble import RandomForestClassifier\n\n    # Initialize model\n    model = RandomForestClassifier(n_estimators=100, warm_start=True)\n\n    # Train in batches\n    for i in range(0, len(X), batch_size):\n        X_batch = X[i:i+batch_size]\n        y_batch = y[i:i+batch_size]\n\n        # Fit model (incrementally)\n        model.fit(X_batch, y_batch)\n\n    return model\n</code></pre></p>"},{"location":"troubleshooting/#model-overfitting","title":"Model Overfitting","text":"<p>Problem: High training accuracy but poor test performance</p> <p>Solution: <pre><code># Use cross-validation\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\n\ndef evaluate_model_robustly(X, y, model):\n    \"\"\"Evaluate model with cross-validation\"\"\"\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n\n    print(f\"Cross-validation scores: {scores}\")\n    print(f\"Mean CV score: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\")\n\n    return scores\n\n# Regularization techniques\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Reduce complexity\nmodel = RandomForestClassifier(\n    n_estimators=50,  # Reduce from 100\n    max_depth=10,     # Limit tree depth\n    min_samples_split=10,  # Increase minimum samples\n    min_samples_leaf=5     # Increase minimum leaf samples\n)\n</code></pre></p>"},{"location":"troubleshooting/#slow-model-training","title":"Slow Model Training","text":"<p>Problem: Training takes too long</p> <p>Solutions:</p> <ol> <li> <p>Reduce model complexity:    <pre><code># Use fewer estimators\nmodel = RandomForestClassifier(n_estimators=50)  # Instead of 100\n</code></pre></p> </li> <li> <p>Use parallel processing:    <pre><code># Use all CPU cores\nmodel = RandomForestClassifier(n_jobs=-1)\n</code></pre></p> </li> <li> <p>Feature selection:    <pre><code>from sklearn.feature_selection import SelectKBest, f_classif\n\n# Select top K features\nselector = SelectKBest(score_func=f_classif, k=50)\nX_selected = selector.fit_transform(X, y)\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#inference-problems","title":"Inference Problems","text":""},{"location":"troubleshooting/#model-loading-errors","title":"Model Loading Errors","text":"<p>Error: <code>FileNotFoundError</code> or model loading fails</p> <p>Solution: <pre><code>import os\nimport joblib\n\ndef safe_load_model(model_path):\n    \"\"\"Safely load model with error handling\"\"\"\n    if not os.path.exists(model_path):\n        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n\n    try:\n        model = joblib.load(model_path)\n        return model\n    except Exception as e:\n        print(f\"Error loading model: {e}\")\n        return None\n\n# Use in your code\nmodel = safe_load_model(\"models/trained_model.pkl\")\nif model is None:\n    print(\"Please train a model first\")\n</code></pre></p>"},{"location":"troubleshooting/#prediction-errors","title":"Prediction Errors","text":"<p>Error: Shape mismatch or prediction fails</p> <p>Solution: <pre><code>def validate_input_shape(X, expected_features):\n    \"\"\"Validate input data shape\"\"\"\n    if X.shape[1] != expected_features:\n        raise ValueError(f\"Expected {expected_features} features, got {X.shape[1]}\")\n\n# Save feature information with model\ndef save_model_with_metadata(model, feature_names, model_path):\n    \"\"\"Save model with feature metadata\"\"\"\n    model_data = {\n        'model': model,\n        'feature_names': feature_names,\n        'n_features': len(feature_names)\n    }\n    joblib.dump(model_data, model_path)\n\ndef load_model_with_metadata(model_path):\n    \"\"\"Load model with feature metadata\"\"\"\n    model_data = joblib.load(model_path)\n    return model_data['model'], model_data['feature_names']\n</code></pre></p>"},{"location":"troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"troubleshooting/#slow-audio-processing","title":"Slow Audio Processing","text":"<p>Problem: Audio processing is too slow</p> <p>Solutions:</p> <ol> <li> <p>Optimize audio loading:    <pre><code># Load only necessary duration\naudio, sr = librosa.load(filename, \n                        duration=30,  # Only load 30 seconds\n                        sr=22050)     # Lower sample rate\n</code></pre></p> </li> <li> <p>Use faster feature extraction:    <pre><code># Use hop_length to reduce computation\nmfccs = librosa.feature.mfcc(y=audio, \n                            sr=sr, \n                            n_mfcc=13,\n                            hop_length=1024)  # Increase hop_length\n</code></pre></p> </li> <li> <p>Batch processing:    <pre><code>def process_files_in_batches(file_list, batch_size=10):\n    \"\"\"Process files in batches\"\"\"\n    for i in range(0, len(file_list), batch_size):\n        batch = file_list[i:i+batch_size]\n        # Process batch\n        yield process_batch(batch)\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#high-memory-usage","title":"High Memory Usage","text":"<p>Problem: Application uses too much memory</p> <p>Solutions:</p> <ol> <li> <p>Clear variables:    <pre><code>import gc\n\n# Clear large variables\ndel large_audio_data\ngc.collect()\n</code></pre></p> </li> <li> <p>Use generators:    <pre><code>def audio_data_generator(file_list):\n    \"\"\"Generate audio data on demand\"\"\"\n    for filename in file_list:\n        audio, sr = librosa.load(filename, sr=None)\n        yield filename, audio\n\n# Use generator instead of loading all files\nfor filename, audio in audio_data_generator(file_list):\n    # Process one file at a time\n    pass\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#documentation-and-development","title":"Documentation and Development","text":""},{"location":"troubleshooting/#mkdocs-build-errors","title":"MkDocs Build Errors","text":"<p>Error: Documentation build fails</p> <p>Solution: <pre><code># Install dependencies\npip install mkdocs mkdocs-material mkdocstrings\n\n# Check for syntax errors\nmkdocs build --strict\n\n# Common fixes\n# 1. Fix markdown syntax errors\n# 2. Check file paths in navigation\n# 3. Verify code block syntax\n</code></pre></p>"},{"location":"troubleshooting/#jupyter-notebook-issues","title":"Jupyter Notebook Issues","text":"<p>Error: Notebooks won't run or display errors</p> <p>Solutions:</p> <ol> <li> <p>Kernel issues:    <pre><code># Install kernel\npython -m ipykernel install --user --name=masterai\n\n# Restart kernel in notebook\n# Kernel -&gt; Restart &amp; Clear Output\n</code></pre></p> </li> <li> <p>Missing dependencies:    <pre><code># Install in notebook\n!pip install missing_package\n</code></pre></p> </li> <li> <p>Path issues:    <pre><code># Add project root to path\nimport sys\nsys.path.append('../')\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":""},{"location":"troubleshooting/#before-asking-for-help","title":"Before Asking for Help","text":"<ol> <li>Check this troubleshooting guide</li> <li>Search existing issues on GitHub</li> <li>Try the minimal example to isolate the problem</li> <li>Check your Python and library versions</li> </ol>"},{"location":"troubleshooting/#creating-a-good-bug-report","title":"Creating a Good Bug Report","text":"<p>When reporting issues, include:</p> <ol> <li> <p>System information:    <pre><code>import sys\nimport librosa\nimport sklearn\nimport tensorflow\n\nprint(f\"Python: {sys.version}\")\nprint(f\"LibROSA: {librosa.__version__}\")\nprint(f\"Scikit-learn: {sklearn.__version__}\")\nprint(f\"TensorFlow: {tensorflow.__version__}\")\n</code></pre></p> </li> <li> <p>Minimal reproduction example:    <pre><code># Provide the smallest code that reproduces the issue\nfrom src.data_processing import load_audio_files\n\n# This fails\naudio_data = load_audio_files(\"path/to/files\")\n</code></pre></p> </li> <li> <p>Error traceback: Copy the complete error message</p> </li> <li> <p>Expected vs actual behavior: Describe what should happen vs what happens</p> </li> </ol>"},{"location":"troubleshooting/#community-resources","title":"Community Resources","text":"<ul> <li>GitHub Issues: Report bugs and request features</li> <li>GitHub Discussions: Ask questions and share ideas</li> <li>Documentation: Read the full documentation</li> <li>Examples: Check the <code>notebooks/</code> directory for working examples</li> </ul>"},{"location":"troubleshooting/#common-workflow-issues","title":"Common Workflow Issues","text":""},{"location":"troubleshooting/#no-module-named-src","title":"\"No module named 'src'\"","text":"<p>Solution: <pre><code># Add project root to Python path\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))\n\n# Or use relative imports\nfrom .src.data_processing import load_audio_files\n</code></pre></p>"},{"location":"troubleshooting/#model-file-not-found","title":"\"Model file not found\"","text":"<p>Solution: <pre><code># Check model directory\nimport os\nif not os.path.exists(\"models/\"):\n    os.makedirs(\"models/\")\n\n# Train model if not exists\nif not os.path.exists(\"models/trained_model.pkl\"):\n    print(\"Training new model...\")\n    # Train and save model\n</code></pre></p>"},{"location":"troubleshooting/#feedback-file-not-found","title":"\"Feedback file not found\"","text":"<p>Solution: <pre><code># Create empty feedback file\nimport json\nif not os.path.exists(\"feedback.json\"):\n    with open(\"feedback.json\", \"w\") as f:\n        pass  # Create empty file\n</code></pre></p> <p>Remember: Most issues are common and solvable! Don't hesitate to ask for help in the community forums.</p>"},{"location":"usage/","title":"Usage Guide","text":""},{"location":"usage/#installation-and-setup","title":"Installation and Setup","text":""},{"location":"usage/#prerequisites","title":"Prerequisites","text":"<p>Before using MasterIA, ensure you have Python 3.8+ installed on your system.</p>"},{"location":"usage/#installation","title":"Installation","text":"<ol> <li> <p>Clone the repository: <pre><code>git clone https://github.com/Esgr0bar/MasterIA.git\ncd MasterIA\n</code></pre></p> </li> <li> <p>Create and activate a virtual environment: <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Install dependencies: <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> </ol>"},{"location":"usage/#quick-start","title":"Quick Start","text":""},{"location":"usage/#1-basic-usage-command-line","title":"1. Basic Usage - Command Line","text":"<p>Run the main application: <pre><code>python main.py\n</code></pre></p> <p>This will: - Load audio data from <code>data/audio_with_metadata/</code> - Extract features and train an initial model - Run inference on new audio files from <code>data/new_audio/</code> - Display suggested actions and cuts - Collect user feedback for model improvement</p>"},{"location":"usage/#2-using-individual-components","title":"2. Using Individual Components","text":"<p>Data Processing: <pre><code>from src.data_processing import load_audio_files_with_metadata\n\n# Load audio files with metadata\ndata_dir = \"data/audio_with_metadata/\"\naudio_data, metadata = load_audio_files_with_metadata(data_dir)\n</code></pre></p> <p>Feature Extraction: <pre><code>from src.feature_extraction import extract_basic_features, extract_mfcc\n\n# Extract basic features\nfeatures = extract_basic_features(audio_data)\n\n# Extract MFCC features\nmfcc_features = extract_mfcc(audio_data, n_mfcc=13)\n</code></pre></p> <p>Model Training: <pre><code>from src.model_training import prepare_data_for_training, train_model\n\n# Prepare data for training\nX, y = prepare_data_for_training(features, metadata)\n\n# Train the model\nmodel = train_model(X, y)\n</code></pre></p> <p>Inference: <pre><code>from src.inference import run_inference\n\n# Run inference on new audio data\nmodel_path = \"models/trained_model.pkl\"\nsuggested_actions, suggested_cuts = run_inference(model_path, new_audio_data)\n</code></pre></p>"},{"location":"usage/#data-preparation","title":"Data Preparation","text":""},{"location":"usage/#audio-file-format","title":"Audio File Format","text":"<p>MasterIA expects audio files in WAV format with accompanying JSON metadata files:</p> <pre><code>data/audio_with_metadata/\n\u251c\u2500\u2500 track1.wav\n\u251c\u2500\u2500 track1.json\n\u251c\u2500\u2500 track2.wav\n\u251c\u2500\u2500 track2.json\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"usage/#metadata-format","title":"Metadata Format","text":"<p>Each audio file should have a corresponding JSON metadata file:</p> <pre><code>{\n  \"title\": \"Track Name\",\n  \"artist\": \"Artist Name\",\n  \"genre\": \"Hip-hop\",\n  \"bpm\": 120,\n  \"key\": \"C major\",\n  \"effects\": [\n    {\n      \"effect\": \"EQ\",\n      \"target\": \"vocals\",\n      \"level\": 0.7\n    },\n    {\n      \"effect\": \"Reverb\",\n      \"target\": \"drums\",\n      \"level\": 0.3\n    }\n  ]\n}\n</code></pre>"},{"location":"usage/#advanced-usage","title":"Advanced Usage","text":""},{"location":"usage/#custom-model-training","title":"Custom Model Training","text":"<p>You can train custom models for specific genres or use cases:</p> <pre><code>from src.model_training import train_model\nfrom src.feature_extraction import extract_basic_features\n\n# Extract features for your specific dataset\nfeatures = extract_basic_features(genre_specific_data)\n\n# Train a genre-specific model\nmodel = train_model(features, genre_labels)\n\n# Save the model\nmodel.save(\"models/genre_specific_model.pkl\")\n</code></pre>"},{"location":"usage/#batch-processing","title":"Batch Processing","text":"<p>Process multiple audio files at once:</p> <pre><code>from src.data_processing import load_audio_files\nfrom src.inference import run_inference\n\n# Load multiple audio files\naudio_files = load_audio_files(\"data/batch_processing/\")\n\n# Process all files\nresults = {}\nfor filename, audio_data in audio_files.items():\n    actions, cuts = run_inference(\"models/trained_model.pkl\", {filename: audio_data})\n    results[filename] = {\"actions\": actions, \"cuts\": cuts}\n</code></pre>"},{"location":"usage/#interactive-mode","title":"Interactive Mode","text":"<p>Use the tool interactively to get real-time feedback:</p> <pre><code>from src.feedback import collect_user_feedback, save_feedback\n\n# Get AI suggestions\nactions, cuts = run_inference(model_path, audio_data)\n\n# Collect user feedback\nfeedback = collect_user_feedback(actions, cuts)\n\n# Save feedback for model improvement\nsave_feedback(feedback)\n</code></pre>"},{"location":"usage/#configuration","title":"Configuration","text":""},{"location":"usage/#environment-variables","title":"Environment Variables","text":"<p>Set these environment variables to customize behavior:</p> <pre><code>export MASTERAI_DATA_DIR=\"/path/to/your/data\"\nexport MASTERAI_MODEL_DIR=\"/path/to/your/models\"\nexport MASTERAI_OUTPUT_DIR=\"/path/to/output\"\n</code></pre>"},{"location":"usage/#model-parameters","title":"Model Parameters","text":"<p>Adjust model parameters in your code:</p> <pre><code># For ensemble model training\nmodel = train_model(\n    features, \n    labels,\n    n_estimators=200,  # Random Forest estimators\n    cnn_epochs=20,     # CNN training epochs\n    test_size=0.2      # Train/test split ratio\n)\n</code></pre>"},{"location":"usage/#output-format","title":"Output Format","text":""},{"location":"usage/#suggested-actions","title":"Suggested Actions","text":"<p>The AI outputs suggested actions in the following format:</p> <pre><code>{\n  \"track1.wav\": [\n    {\n      \"effect\": \"EQ\",\n      \"target\": \"vocals\",\n      \"level\": 0.8,\n      \"frequency\": \"high\"\n    },\n    {\n      \"effect\": \"Compression\",\n      \"target\": \"drums\",\n      \"level\": 0.6,\n      \"ratio\": \"4:1\"\n    }\n  ]\n}\n</code></pre>"},{"location":"usage/#suggested-cuts","title":"Suggested Cuts","text":"<p>Creative cuts and edits are suggested as:</p> <pre><code>{\n  \"track1.wav\": [\n    {\n      \"action\": \"Cut\",\n      \"location\": \"Chorus Start\",\n      \"description\": \"Introduce a glitch effect\",\n      \"timestamp\": \"0:45\"\n    },\n    {\n      \"action\": \"Slice\",\n      \"location\": \"Verse Mid\",\n      \"description\": \"Add a stutter effect\",\n      \"timestamp\": \"1:23\"\n    }\n  ]\n}\n</code></pre>"},{"location":"usage/#troubleshooting","title":"Troubleshooting","text":""},{"location":"usage/#common-issues","title":"Common Issues","text":"<p>1. Audio file not loading: <pre><code>Error: Could not load audio file\nSolution: Ensure the file is in WAV format and not corrupted\n</code></pre></p> <p>2. Model training fails: <pre><code>Error: Insufficient training data\nSolution: Ensure you have at least 10 audio files with metadata\n</code></pre></p> <p>3. Memory issues: <pre><code>Error: Out of memory during feature extraction\nSolution: Process files in smaller batches or reduce audio length\n</code></pre></p>"},{"location":"usage/#performance-tips","title":"Performance Tips","text":"<ol> <li>Use smaller audio segments (5-10 seconds) for faster processing</li> <li>Preprocess audio files to consistent sample rates (44.1kHz recommended)</li> <li>Use GPU acceleration for CNN model training (if available)</li> <li>Cache extracted features to avoid recomputation</li> </ol>"},{"location":"usage/#examples","title":"Examples","text":""},{"location":"usage/#example-1-basic-rap-track-processing","title":"Example 1: Basic Rap Track Processing","text":"<pre><code># Load a rap track\naudio_data, metadata = load_audio_files_with_metadata(\"data/rap_tracks/\")\n\n# Extract features\nfeatures = extract_basic_features(audio_data)\n\n# Get AI suggestions\nactions, cuts = run_inference(\"models/rap_model.pkl\", audio_data)\n\n# Print suggestions\nfor track, track_actions in actions.items():\n    print(f\"Track: {track}\")\n    for action in track_actions:\n        print(f\"  - Apply {action['effect']} to {action['target']} at level {action['level']}\")\n</code></pre>"},{"location":"usage/#example-2-custom-effect-analysis","title":"Example 2: Custom Effect Analysis","text":"<pre><code># Analyze specific effects in your tracks\nfrom src.feature_extraction import extract_mfcc\n\n# Extract detailed features\nmfcc_features = extract_mfcc(audio_data, n_mfcc=25)\n\n# Train a model specifically for effect recognition\neffect_model = train_model(mfcc_features, effect_labels)\n\n# Use the model to suggest similar effects\nsuggestions = effect_model.predict(new_track_features)\n</code></pre> <p>For more examples, check out the Jupyter notebooks in the <code>notebooks/</code> directory.</p>"}]}