{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AI-Based Audio Mixing and Mastering","text":""},{"location":"#overview","title":"Overview","text":"<p>This project aims to develop an AI-powered tool for automated audio mixing and mastering. The tool leverages machine learning algorithms to analyze and replicate the effects applied to reference tracks, enabling rapid processing of audio files.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Automated Mixing and Mastering: AI-driven analysis and application of audio effects.</li> <li>Flexible Adjustment: Users can choose from different effect intensities and manually tweak the results.</li> <li>Support for Various Genres: The AI can be calibrated to handle different music genres, starting with rap.</li> </ul>"},{"location":"#project-structure","title":"Project Structure","text":"<ul> <li>Data Processing: Functions for loading and preparing audio data.</li> <li>Feature Extraction: Methods to extract useful features like MFCCs and spectrograms.</li> <li>Model Training: Scripts to build, train, and evaluate machine learning models.</li> <li>User Interface: A simple application interface (to be developed).</li> </ul> <p>Explore the documentation to learn more about how to use the tool and contribute to the project.</p>"},{"location":"api_reference/","title":"API Reference","text":"<p>This section contains the detailed API documentation for the project's Python modules. </p>"},{"location":"api_reference/#data-processing","title":"Data Processing","text":"<p>Data processing utilities for audio files.</p> <p>This module provides functions to load and split audio files into segments.</p> <p>Author: Esgr0bar</p>"},{"location":"api_reference/#src.data_processing.load_audio","title":"<code>load_audio(file_path, sr=44100)</code>","text":"<p>Load an audio file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the audio file.</p> required <code>sr</code> <code>int</code> <p>Sample rate for loading audio.</p> <code>44100</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>Tuple containing the audio time series (numpy.ndarray) and sample rate (int).</p>"},{"location":"api_reference/#src.data_processing.split_tracks","title":"<code>split_tracks(file_path, segment_length=5)</code>","text":"<p>Split an audio track into smaller segments.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the audio file.</p> required <code>segment_length</code> <code>int</code> <p>Length of each segment in seconds.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of audio segments (numpy.ndarray).</p>"},{"location":"api_reference/#feature-extraction","title":"Feature Extraction","text":"<p>Feature extraction utilities for audio processing.</p> <p>This module provides functions to extract features such as MFCCs and spectrograms.</p> <p>Author: Esgr0bar</p>"},{"location":"api_reference/#src.feature_extraction.extract_mfcc","title":"<code>extract_mfcc(y, sr, n_mfcc=13)</code>","text":"<p>Extract MFCC features from an audio signal.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ndarray</code> <p>Audio time series.</p> required <code>sr</code> <code>int</code> <p>Sample rate of the audio.</p> required <code>n_mfcc</code> <code>int</code> <p>Number of MFCCs to return.</p> <code>13</code> <p>Returns:</p> Type Description <p>numpy.ndarray: MFCC feature matrix.</p>"},{"location":"api_reference/#src.feature_extraction.extract_spectrogram","title":"<code>extract_spectrogram(y, sr, n_fft=2048, hop_length=512)</code>","text":"<p>Extract spectrogram from an audio signal.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ndarray</code> <p>Audio time series.</p> required <code>sr</code> <code>int</code> <p>Sample rate of the audio.</p> required <code>n_fft</code> <code>int</code> <p>Number of samples per FFT.</p> <code>2048</code> <code>hop_length</code> <code>int</code> <p>Number of samples between successive frames.</p> <code>512</code> <p>Returns:</p> Type Description <p>numpy.ndarray: Spectrogram matrix.</p>"},{"location":"api_reference/#model-training","title":"Model Training","text":"<p>Model training utilities for AI-based audio processing.</p> <p>This module provides functions to build, train, and save machine learning models.</p> <p>Author: Esgr0bar</p>"},{"location":"api_reference/#src.model_training.build_model","title":"<code>build_model(input_shape)</code>","text":"<p>Build a Convolutional Neural Network (CNN) model.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>tuple</code> <p>Shape of the input data (height, width, channels).</p> required <p>Returns:</p> Type Description <p>keras.Model: Compiled CNN model.</p>"},{"location":"api_reference/#src.model_training.save_model","title":"<code>save_model(model, file_path)</code>","text":"<p>Save the trained model to a file.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>Trained model.</p> required <code>file_path</code> <code>str</code> <p>Path to save the model.</p> required <p>Returns:</p> Type Description <p>None</p>"},{"location":"api_reference/#src.model_training.train_model","title":"<code>train_model(model, X_train, y_train, epochs=10, validation_split=0.2)</code>","text":"<p>Train the CNN model on the provided data.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>Compiled model.</p> required <code>X_train</code> <code>ndarray</code> <p>Training data.</p> required <code>y_train</code> <code>ndarray</code> <p>Target values.</p> required <code>epochs</code> <code>int</code> <p>Number of epochs to train.</p> <code>10</code> <code>validation_split</code> <code>float</code> <p>Fraction of data to use for validation.</p> <code>0.2</code> <p>Returns:</p> Type Description <p>keras.callbacks.History: Training history.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p>"},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Initial project structure with <code>data_processing.py</code>, <code>feature_extraction.py</code>, and <code>model_training.py</code>.</li> <li>Basic unit tests for all modules.</li> <li>Jupyter notebooks for EDA and model training.</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>N/A</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>N/A</li> </ul>"},{"location":"changelog/#010-2024-08-09","title":"[0.1.0] - 2024-08-09","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Initial setup of <code>mkdocs</code> for project documentation.</li> </ul>"},{"location":"ci_cd/","title":"Continuous Integration and Deployment","text":""},{"location":"ci_cd/#overview","title":"Overview","text":"<p>We use continuous integration (CI) to automatically test and deploy our project. This ensures that new changes do not break existing functionality and that our documentation is always up-to-date.</p>"},{"location":"ci_cd/#github-actions","title":"GitHub Actions","text":"<p>Our project is set up with GitHub Actions for CI/CD. The following workflows are defined:</p> <ul> <li>Test Workflow: Automatically runs the unit tests defined in the <code>tests/</code> directory.</li> <li>Documentation Deployment: Deploys the <code>mkdocs</code> documentation to GitHub Pages whenever changes are pushed to the <code>main</code> branch.</li> </ul>"},{"location":"ci_cd/#setting-up-cicd","title":"Setting Up CI/CD","text":"<ol> <li>Test Workflow:</li> <li>The test workflow runs <code>pytest</code> to execute all unit tests.</li> <li> <p>Ensure all tests pass before merging to <code>main</code>.</p> </li> <li> <p>Documentation Deployment:</p> </li> <li><code>mkdocs gh-deploy</code> is used to deploy the latest documentation to GitHub Pages.</li> </ol>"},{"location":"ci_cd/#workflow-files","title":"Workflow Files","text":"<p>You can find the CI workflow files in the <code>.github/workflows/</code> directory.</p> <p>For more details on setting up and customizing CI/CD workflows, refer to the GitHub Actions Documentation.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>We welcome contributions to improve this project. Whether you want to report a bug, suggest an enhancement, or submit code, here\u2019s how you can contribute:</p>"},{"location":"contributing/#how-to-contribute","title":"How to Contribute","text":"<ol> <li>Fork the Repository:</li> <li> <p>Fork the project on GitHub to your account.</p> </li> <li> <p>Clone the Repository:</p> </li> <li> <p>Clone your fork locally with:      <code>bash      git clone https://github.com/your-username/your-repo-name.git</code></p> </li> <li> <p>Create a New Branch:</p> </li> <li> <p>Create a branch for your changes:      <code>bash      git checkout -b feature/my-feature</code></p> </li> <li> <p>Make Changes:</p> </li> <li>Implement your changes or additions.</li> <li> <p>Add tests if applicable.</p> </li> <li> <p>Commit and Push:</p> </li> <li> <p>Commit your changes and push them to your fork:      <code>bash      git commit -m \"Description of changes\"      git push origin feature/my-feature</code></p> </li> <li> <p>Create a Pull Request:</p> </li> <li>Submit a pull request to the main repository.</li> <li>Ensure your pull request description clearly explains the changes and references any relevant issues.</li> </ol> <p>Thank you for your contributions!</p>"},{"location":"data/","title":"Data Handling","text":""},{"location":"data/#data-structure","title":"Data Structure","text":"<p>The project works with audio data stored in a specific directory structure:</p> <ul> <li>Raw Data: Unprocessed audio files, typically in <code>.wav</code> format.</li> <li><code>data/raw/tracks/</code>: Contains folders for each track (e.g., <code>song1/</code>, <code>song2/</code>).</li> <li> <p>Example: <code>data/raw/tracks/song1/vocals.wav</code></p> </li> <li> <p>Processed Data: Feature files and preprocessed data ready for model input.</p> </li> <li><code>data/processed/features/</code>: Contains extracted features like MFCCs and spectrograms.</li> <li>Example: <code>data/processed/features/song1_vocals_mfcc.npy</code></li> </ul>"},{"location":"data/#data-collection","title":"Data Collection","text":"<ul> <li>Source Audio: Collect raw audio files for different tracks (vocals, instruments, etc.).</li> <li>Reference Tracks: Include mixed and mastered tracks for the AI to learn from.</li> </ul>"},{"location":"data/#data-processing-workflow","title":"Data Processing Workflow","text":"<ol> <li>Loading Audio:</li> <li> <p>Use the <code>load_audio()</code> function from <code>src/data_processing.py</code> to load audio files into numpy arrays.</p> </li> <li> <p>Splitting Tracks:</p> </li> <li> <p>Use the <code>split_tracks()</code> function to divide longer audio files into manageable segments.</p> </li> <li> <p>Feature Extraction:</p> </li> <li> <p>Use <code>extract_mfcc()</code> and <code>extract_spectrogram()</code> from <code>src/feature_extraction.py</code> to extract features for model training.</p> </li> <li> <p>Saving Processed Data:</p> </li> <li>Save extracted features as <code>.npy</code> files in the <code>data/processed/features/</code> directory.</li> </ol> <p>Ensure your data is properly organized before starting model training.</p>"},{"location":"notebooks/","title":"Jupyter Notebooks","text":""},{"location":"notebooks/#overview","title":"Overview","text":"<p>The Jupyter notebooks included in this project are designed for data exploration and model training. They provide an interactive environment where you can test different approaches and visualize the results.</p>"},{"location":"notebooks/#available-notebooks","title":"Available Notebooks","text":"<ul> <li>EDA.ipynb: Explore the raw audio data, visualize waveforms, spectrograms, and MFCCs.</li> <li>Model_Training.ipynb: Build and train machine learning models for audio processing.</li> </ul>"},{"location":"notebooks/#how-to-use","title":"How to Use","text":"<ol> <li>Set Up Your Environment:</li> <li>Ensure that you have all dependencies installed by running <code>pip install -r requirements.txt</code>.</li> <li> <p>Start Jupyter with the command <code>jupyter notebook</code>.</p> </li> <li> <p>Open a Notebook:</p> </li> <li>Navigate to the <code>notebooks/</code> directory.</li> <li> <p>Open <code>EDA.ipynb</code> to start exploring the data or <code>Model_Training.ipynb</code> to begin training models.</p> </li> <li> <p>Run Cells:</p> </li> <li>Execute code cells by selecting them and pressing <code>Shift + Enter</code>.</li> <li> <p>Modify parameters as needed to experiment with different settings.</p> </li> <li> <p>Save Your Work:</p> </li> <li>Don\u2019t forget to save your notebook regularly. You can also export it as HTML or PDF for sharing.</li> </ol> <p>Explore the notebooks to get hands-on experience with the data and the models!</p>"},{"location":"reference/data_processing/","title":"Data Processing","text":"<p>Data processing utilities for audio files.</p> <p>This module provides functions to load and split audio files into segments.</p> <p>Author: Esgr0bar</p>"},{"location":"reference/data_processing/#src.data_processing.load_audio","title":"<code>load_audio(file_path, sr=44100)</code>","text":"<p>Load an audio file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the audio file.</p> required <code>sr</code> <code>int</code> <p>Sample rate for loading audio.</p> <code>44100</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>Tuple containing the audio time series (numpy.ndarray) and sample rate (int).</p> Source code in <code>src/data_processing.py</code> <pre><code>def load_audio(file_path, sr=44100):\n    \"\"\"\n    Load an audio file.\n\n    Args:\n        file_path (str): Path to the audio file.\n        sr (int): Sample rate for loading audio.\n\n    Returns:\n        tuple: Tuple containing the audio time series (numpy.ndarray) and sample rate (int).\n    \"\"\"\n    y, sr = librosa.load(file_path, sr=sr)\n    return y, sr\n</code></pre>"},{"location":"reference/data_processing/#src.data_processing.split_tracks","title":"<code>split_tracks(file_path, segment_length=5)</code>","text":"<p>Split an audio track into smaller segments.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the audio file.</p> required <code>segment_length</code> <code>int</code> <p>Length of each segment in seconds.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of audio segments (numpy.ndarray).</p> Source code in <code>src/data_processing.py</code> <pre><code>def split_tracks(file_path, segment_length=5):\n    \"\"\"\n    Split an audio track into smaller segments.\n\n    Args:\n        file_path (str): Path to the audio file.\n        segment_length (int): Length of each segment in seconds.\n\n    Returns:\n        list: A list of audio segments (numpy.ndarray).\n    \"\"\"\n    y, sr = load_audio(file_path)\n    segments = [y[i:i + sr * segment_length] for i in range(0, len(y), sr * segment_length)]\n    return segments\n</code></pre>"},{"location":"reference/feature_extraction/","title":"Feature Extraction","text":"<p>Feature extraction utilities for audio processing.</p> <p>This module provides functions to extract features such as MFCCs and spectrograms.</p> <p>Author: Esgr0bar</p>"},{"location":"reference/feature_extraction/#src.feature_extraction.extract_mfcc","title":"<code>extract_mfcc(y, sr, n_mfcc=13)</code>","text":"<p>Extract MFCC features from an audio signal.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ndarray</code> <p>Audio time series.</p> required <code>sr</code> <code>int</code> <p>Sample rate of the audio.</p> required <code>n_mfcc</code> <code>int</code> <p>Number of MFCCs to return.</p> <code>13</code> <p>Returns:</p> Type Description <p>numpy.ndarray: MFCC feature matrix.</p> Source code in <code>src/feature_extraction.py</code> <pre><code>def extract_mfcc(y, sr, n_mfcc=13):\n    \"\"\"\n    Extract MFCC features from an audio signal.\n\n    Args:\n        y (numpy.ndarray): Audio time series.\n        sr (int): Sample rate of the audio.\n        n_mfcc (int): Number of MFCCs to return.\n\n    Returns:\n        numpy.ndarray: MFCC feature matrix.\n    \"\"\"\n    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n    return mfccs\n</code></pre>"},{"location":"reference/feature_extraction/#src.feature_extraction.extract_spectrogram","title":"<code>extract_spectrogram(y, sr, n_fft=2048, hop_length=512)</code>","text":"<p>Extract spectrogram from an audio signal.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ndarray</code> <p>Audio time series.</p> required <code>sr</code> <code>int</code> <p>Sample rate of the audio.</p> required <code>n_fft</code> <code>int</code> <p>Number of samples per FFT.</p> <code>2048</code> <code>hop_length</code> <code>int</code> <p>Number of samples between successive frames.</p> <code>512</code> <p>Returns:</p> Type Description <p>numpy.ndarray: Spectrogram matrix.</p> Source code in <code>src/feature_extraction.py</code> <pre><code>def extract_spectrogram(y, sr, n_fft=2048, hop_length=512):\n    \"\"\"\n    Extract spectrogram from an audio signal.\n\n    Args:\n        y (numpy.ndarray): Audio time series.\n        sr (int): Sample rate of the audio.\n        n_fft (int): Number of samples per FFT.\n        hop_length (int): Number of samples between successive frames.\n\n    Returns:\n        numpy.ndarray: Spectrogram matrix.\n    \"\"\"\n    S = np.abs(librosa.stft(y, n_fft=n_fft, hop_length=hop_length))\n    return S\n</code></pre>"},{"location":"reference/model_training/","title":"Model Training","text":"<p>Model training utilities for AI-based audio processing.</p> <p>This module provides functions to build, train, and save machine learning models.</p> <p>Author: Esgr0bar</p>"},{"location":"reference/model_training/#src.model_training.build_model","title":"<code>build_model(input_shape)</code>","text":"<p>Build a Convolutional Neural Network (CNN) model.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>tuple</code> <p>Shape of the input data (height, width, channels).</p> required <p>Returns:</p> Type Description <p>keras.Model: Compiled CNN model.</p> Source code in <code>src/model_training.py</code> <pre><code>def build_model(input_shape):\n    \"\"\"\n    Build a Convolutional Neural Network (CNN) model.\n\n    Args:\n        input_shape (tuple): Shape of the input data (height, width, channels).\n\n    Returns:\n        keras.Model: Compiled CNN model.\n    \"\"\"\n    model = models.Sequential([\n        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n        layers.MaxPooling2D((2, 2)),\n        layers.Conv2D(64, (3, 3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n        layers.Conv2D(64, (3, 3), activation='relu'),\n        layers.Flatten(),\n        layers.Dense(64, activation='relu'),\n        layers.Dense(1)  # Assuming regression output; modify for classification\n    ])\n\n    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n    return model\n</code></pre>"},{"location":"reference/model_training/#src.model_training.save_model","title":"<code>save_model(model, file_path)</code>","text":"<p>Save the trained model to a file.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>Trained model.</p> required <code>file_path</code> <code>str</code> <p>Path to save the model.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>src/model_training.py</code> <pre><code>def save_model(model, file_path):\n    \"\"\"\n    Save the trained model to a file.\n\n    Args:\n        model (keras.Model): Trained model.\n        file_path (str): Path to save the model.\n\n    Returns:\n        None\n    \"\"\"\n    model.save(file_path)\n</code></pre>"},{"location":"reference/model_training/#src.model_training.train_model","title":"<code>train_model(model, X_train, y_train, epochs=10, validation_split=0.2)</code>","text":"<p>Train the CNN model on the provided data.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>Compiled model.</p> required <code>X_train</code> <code>ndarray</code> <p>Training data.</p> required <code>y_train</code> <code>ndarray</code> <p>Target values.</p> required <code>epochs</code> <code>int</code> <p>Number of epochs to train.</p> <code>10</code> <code>validation_split</code> <code>float</code> <p>Fraction of data to use for validation.</p> <code>0.2</code> <p>Returns:</p> Type Description <p>keras.callbacks.History: Training history.</p> Source code in <code>src/model_training.py</code> <pre><code>def train_model(model, X_train, y_train, epochs=10, validation_split=0.2):\n    \"\"\"\n    Train the CNN model on the provided data.\n\n    Args:\n        model (keras.Model): Compiled model.\n        X_train (numpy.ndarray): Training data.\n        y_train (numpy.ndarray): Target values.\n        epochs (int): Number of epochs to train.\n        validation_split (float): Fraction of data to use for validation.\n\n    Returns:\n        keras.callbacks.History: Training history.\n    \"\"\"\n    history = model.fit(X_train, y_train, epochs=epochs, validation_split=validation_split)\n    return history\n</code></pre>"}]}